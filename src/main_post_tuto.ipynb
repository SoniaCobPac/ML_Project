{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Soni - do**\r\n",
    "## **Generating Music with Machine Learning**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Sonia Cobo\n",
    "#### Date: July 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though this project doesn't have a hypothesis per se, it was done to kind off prove how AI has advanced and it is now able to generate music which has been associated with emotions and human capabilities for a long period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation - dividir canciones, modificarlas para tener mas datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input to the model will be a series of notes from a MIDI file. MIDI (Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors that connect a wide variety of electronic musical instruments and computers. They don't contain actual audio data and are small in size. They explain what notes are played, when they're played, and how long or loud each note should be.\r\n",
    "\r\n",
    "### To keep the project simple only files with one instrument were chosen, in this case the instrument is piano and the type of songs is classical. \r\n",
    "### These songs have been obtained from the following datasets: http://www.piano-midi.de/ and https://www.mfiles.co.uk/classical-midi.htm\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no descargardas aun: https://github.com/Skuldur/Classical-Piano-Composer/tree/master/midi_songs\n",
    "# https://drive.google.com/file/d/1qnQVK17DNVkU19MgVA4Vg88zRDvwCRXw/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# data manipulation\r\n",
    "import numpy as np\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "# manipulate midi files\r\n",
    "import glob\r\n",
    "from music21 import converter, instrument, note, chord, meter, stream, duration, corpus\r\n",
    "import pygame\r\n",
    "\r\n",
    "# visualization\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# route files\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "# ml model\r\n",
    "import pickle\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "from keras.utils import np_utils\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Activation\r\n",
    "from keras.layers import BatchNormalization as BatchNorm\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "from keras.layers import Bidirectional\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7525799484492965909\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\r\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The route of this file is added to the sys path to be able to import/export functions\n",
    "sep = os.sep\n",
    "def route (steps):\n",
    "    \"\"\"\n",
    "    This function appends the route of the file to the sys path\n",
    "    to be able to import files from/to other foders within the EDA project folder.\n",
    "    \"\"\"\n",
    "    route = os.getcwd()\n",
    "    for i in range(steps):\n",
    "        route = os.path.dirname(route)\n",
    "    sys.path.append(route)\n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\r\n",
    "path = route(1) + sep + \"data\" + sep + \"raw_data\" + sep\r\n",
    "path_1 = route(1) + sep + \"data\" + sep + \"converted_data\" + sep\r\n",
    "path_2 = route(1) + sep + \"data\" + sep + \"notes\" + sep\r\n",
    "path_3 = route(1) + sep + \"models\" + sep\r\n",
    "path_4 = route(1) + sep + \"reports\" + sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midi file exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hablar de frecuencia y la transpuesta de fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All information from the midi file (i.e. notes, pitch, chord, time signature, etc) is contained within the component list\r\n",
    "def info_midi (path):\r\n",
    "    \"\"\"\r\n",
    "    It returns all midi file information given its path\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    file = converter.parse(path)\r\n",
    "    components = []\r\n",
    "    for element in file.recurse():  \r\n",
    "        components.append(element)\r\n",
    "    return components\r\n",
    "\r\n",
    "components = info_midi(path + \"alb_esp1.mid\")\r\n",
    "#components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects stored in a Stream are generally spaced in time; each stored object has an offset usually representing how many quarter notes \r\n",
    "# lies from the beginning of the Stream. For instance in a 4/4 measure of two half notes, the first note will be at offset 0.0,  \r\n",
    "# and the second at offset 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the midi file has been studied and its structure is known, data will be split into two object types: notes, rests and chords. \r\n",
    "\r\n",
    "### Note objects contain information about the pitch, octave, and offset of the note.\r\n",
    "### Pitch refers to the frequency of the sound, or how high or low it is and is represented with the letters [A, B, C, D, E, F, G].\r\n",
    "### Octave refers to which set of pitches you use on a piano.\r\n",
    "### Offset refers to where the note is located in the piece.\r\n",
    "### Rests are the silences in the piece.\r\n",
    "### Chord objects are a set of notes that are played at the same time.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songs are transposed into C major and A minor key to ease predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(path, path_1):\r\n",
    "    \"\"\"\r\n",
    "    This function returns MIDI files converted to C major or A minor key.\r\n",
    "\r\n",
    "    Params: Path of the original MIDI file and path where to save the converted file.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    import music21\r\n",
    "\r\n",
    "    # major conversions\r\n",
    "    majors = dict([(\"A-\", 4),(\"G#\", 4),(\"A\", 3),(\"A#\", 2),(\"B-\", 2),(\"B\", 1),(\"C\", 0),(\"C#\", -1),(\"D-\", -1),(\"D\", -2),(\"D#\", -3),(\"E-\", -3),(\"E\", -4),(\"F\", -5),(\"F#\", 6),(\"G-\", 6),(\"G\", 5)])\r\n",
    "    minors = dict([(\"G#\", 1), (\"A-\", 1),(\"A\", 0),(\"A#\", -1),(\"B-\", -1),(\"B\", -2),(\"C\", -3),(\"C#\", -4),(\"D-\", -4),(\"D\", -5),(\"D#\", 6),(\"E-\", 6),(\"E\", 5),(\"F\", 4),(\"F#\", 3),(\"G-\", 3),(\"G\", 2)])       \r\n",
    "\r\n",
    "    # os.chdir(\"./\")\r\n",
    "    for file in glob.glob(path + \"*.mid\"):\r\n",
    "        score = music21.converter.parse(file)\r\n",
    "        key = score.analyze('key')\r\n",
    "        \r\n",
    "        # print key.tonic.name, key.mode\r\n",
    "        if key.mode == \"major\":\r\n",
    "            halfSteps = majors[key.tonic.name]\r\n",
    "            \r\n",
    "        elif key.mode == \"minor\":\r\n",
    "            halfSteps = minors[key.tonic.name]\r\n",
    "        \r\n",
    "        newscore = score.transpose(halfSteps)\r\n",
    "        key = newscore.analyze(\"key\")\r\n",
    "\r\n",
    "        #print(key.tonic.name, key.mode)\r\n",
    "        newFileName = \"C_\" + file[61:]\r\n",
    "        newscore.write(\"midi\", path_1 + newFileName)\r\n",
    "\r\n",
    "convert_to_midi(path, path_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant information from midi file is encoded and saved into an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We append the pitch of every note object using its string notation since the most significant parts of the note can be recreated using the string notation of the pitch. And we append every chord by encoding the id of every note in the chord together into a single string, with each note being separated by a dot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each midi file contains notes and chords. These two properties will be the input and output of the LSTM network so \r\n",
    "# they need to be taken out from all midi files. \r\n",
    "\r\n",
    "def get_notes_per_song(path, filename):\r\n",
    "    \"\"\"\r\n",
    "    This function extracts all the notes, rests and chords from one midi file\r\n",
    "    and saves it in a list in the converted_data folder.\r\n",
    "\r\n",
    "    Param: Path of the midi file, filename (str)\r\n",
    "    \"\"\"\r\n",
    "    components = info_midi(path + filename)\r\n",
    "    note_list = []\r\n",
    "    \r\n",
    "    for element in components:\r\n",
    "        # note pitches are extracted\r\n",
    "        if isinstance(element, note.Note):\r\n",
    "            note_list.append(str(element.pitch))\r\n",
    "        # chords are extracted\r\n",
    "        elif isinstance(element, chord.Chord):\r\n",
    "            note_list.append(\".\".join(str(n) for n in element.normalOrder))    \r\n",
    "        # rests are extracted\r\n",
    "        elif isinstance(element, note.Rest):\r\n",
    "            note_list.append(\"NULL\")    #further transformation needs this value as str rather than np.nan\r\n",
    "\r\n",
    "    with open(path_2 + \"notes\", \"wb\") as filepath:\r\n",
    "        pickle.dump(note_list, filepath)\r\n",
    "    \r\n",
    "    return note_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_list = get_notes_per_song(path_1, \"C_alb_esp1.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(note_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load notes and chords previously separated\r\n",
    "def load_notes (path, filename):\r\n",
    "    \"\"\"\r\n",
    "    Load the note list containing pitches, rests and chords.\r\n",
    "    \r\n",
    "    Param: Path of the saved note list, and its name as string\r\n",
    "    \"\"\"\r\n",
    "    with open(path + filename, \"rb\") as f:\r\n",
    "        loaded_notes = pickle.load(f)\r\n",
    "        return loaded_notes\r\n",
    "\r\n",
    "note_list = load_notes(path_2, \"notes\")\r\n",
    "#note_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model will be first trained with a small proportioned of the songs to expedite time. Once the model is tunned properly all songs will be passed to improve its training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that all notes, rests and chords are in a list, these will be transformed from categorical data to integer-based numerical data. It is necessary to create input sequences for the network and their respective outputs. The output for each input sequence will be the first note or chord that comes after the sequence of notes in the input sequence in our list of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, sequence_length, step):\r\n",
    "    \"\"\" \r\n",
    "    Prepare the sequences used by the neural network \r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # get all pitch names\r\n",
    "    #pitchnames = sorted(set(item for item in notes)) \r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    print('Total unique notes:', len(pitchnames))\r\n",
    "\r\n",
    "    # create a dictionary to convert pitches (strings) to integers\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\r\n",
    "    # (rests are included)   \r\n",
    "\r\n",
    "    network_input = []\r\n",
    "    network_output = []\r\n",
    "\r\n",
    "    #sequence_in = []\r\n",
    "    #sequence_out = []\r\n",
    "    # create input sequences and the corresponding outputs\r\n",
    "    for i in range(0, len(notes) - sequence_length, step):    \r\n",
    "        network_input.append(notes[i:i + sequence_length])\r\n",
    "        network_output.append(notes[i + sequence_length])\r\n",
    "        # exchange their values for their integer-code\r\n",
    "        #network_input.append([note_to_int[elem] for elem in sequence_in])\r\n",
    "        #network_output.append(note_to_int[sequence_out])\r\n",
    "\r\n",
    "    x = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    y = np.zeros((len(network_input), len(pitchnames)))\r\n",
    "    for i, sequence in enumerate(network_input):\r\n",
    "        for j, note in enumerate(sequence):\r\n",
    "            x[i, j, note_to_int[note]] = 1\r\n",
    "        y[i, note_to_int[network_output[i]]] = 1\r\n",
    "\r\n",
    "    #n_patterns = len(network_input)\r\n",
    "\r\n",
    "    # reshape the input into a format compatible with LSTM layers\r\n",
    "    #network_input = np.reshape(network_input, (n_patterns, sequence_length, 1)) \r\n",
    "    # normalize input\r\n",
    "    #network_input = network_input / float(len(set(notes)))  \r\n",
    "\r\n",
    "    #network_output = np_utils.to_categorical(network_output) # used to convert array of labeled data to one-hot vector\r\n",
    "\r\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The length of each sequence will be 100 notes/chords for now. This means that to predict the next note in the sequence the network has the previous 100 notes to help make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique notes: 50\n"
     ]
    }
   ],
   "source": [
    "x, y = prepare_sequences(notes=note_list, sequence_length=100, step=3)  # length y step pueden variar  \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 100, 50)\n",
      "(196, 50)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\r\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four different types of layers:\n",
    "\n",
    "LSTM layers is a Recurrent Neural Net layer that takes a sequence as an input and can return either sequences (return_sequences=True) or a matrix.\n",
    "\n",
    "Dropout layers are a regularisation technique that consists of setting a fraction of input units to 0 at each update during the training to prevent overfitting. The fraction is determined by the parameter used with the layer.\n",
    "\n",
    "Dense layers or fully connected layers is a fully connected neural network layer where each input node is connected to each output node.\n",
    "\n",
    "The Activation layer determines what activation function our neural network will use to calculate the output of a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network():\r\n",
    "    pitchnames = len(set(note_list))\r\n",
    "\r\n",
    "    model = Sequential()\r\n",
    "    model.add(LSTM(512, input_shape=(100,50)))\r\n",
    "    model.add(Dense(50))\r\n",
    "    model.add(Activation(\"softmax\"))\r\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\r\n",
    "\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 14s 375ms/step - loss: 4.0040\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 3s 374ms/step - loss: 3.3438\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 3s 372ms/step - loss: 3.2140\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 3s 369ms/step - loss: 3.0658\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 3s 375ms/step - loss: 3.0402\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 3s 372ms/step - loss: 2.9484\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 3s 370ms/step - loss: 3.0115\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 3s 376ms/step - loss: 2.9082\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 3s 369ms/step - loss: 3.1540\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 3s 368ms/step - loss: 2.8951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296b86011c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=10)#, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\r\n",
    "model.save(path_3 + \"model_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model \r\n",
    "model_4 = tf.keras.models.load_model(path_3 + \"model_5.h5\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\r\n",
    "    # helper function to sample an index from a probability array\r\n",
    "    preds = np.asarray(preds).astype(\"float64\")\r\n",
    "    preds = np.log(preds) / temperature\r\n",
    "    exp_preds = np.exp(preds)\r\n",
    "    preds = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, preds, 1)\r\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, x, temperature=1.0):\r\n",
    "    \"\"\" \r\n",
    "    Generate notes from the neural network based on a sequence of notes \r\n",
    "    \"\"\"\r\n",
    "    # pick a random sequence from the input as a starting point for the prediction\r\n",
    "    start = np.random.randint(0, len(note_list)-100-1)\r\n",
    "\r\n",
    "    pitchnames = sorted(set(note_list))\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \r\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\r\n",
    "\r\n",
    "    pattern = x[start: start+100]\r\n",
    "    prediction_output = []\r\n",
    "    patterns = []\r\n",
    "    # generate 500 notes, roughly two minutes of music\r\n",
    "    for note_index in range(200):\r\n",
    "        prediction_input = np.zeros((1, 100, len(pitchnames)))\r\n",
    "        for j, note in enumerate(pattern):\r\n",
    "            prediction_input[0, j, note_to_int[note]] = 1.0\r\n",
    "        preds = model.predict(prediction_input, verbose=0)[0]   #[0]?\r\n",
    "        next_index = sample(preds, temperature=temperature)\r\n",
    "        next_note = int_to_note[next_index]\r\n",
    "        prediction_output.append(next_note)\r\n",
    "\r\n",
    "        patterns.append(next_index)\r\n",
    "        #patterns = patterns[1:len(patterns)]\r\n",
    "\r\n",
    "    return prediction_output, patterns, next_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "2 []\n",
      "['G3', 'A3', 'B3', 'C4', 'F3', 'G3', 'G3', 'G#3', 'D4', 'B-3', 'G3', 'A3', 'G3', 'A3', 'D4', 'NULL', 'E6', 'A5', 'E3', 'F3', 'C3', 'G3', 'NULL', 'NULL', 'NULL', 'B3', 'A3', 'G3', 'B3', 'G3', 'G3', 'G#3', 'F4', 'B3', 'G#3', 'A3', 'G3', 'E5', 'NULL', 'D4', 'G#3', 'NULL', 'B3', 'C6', 'A3', 'NULL', 'C4', 'E3', 'NULL', 'F3', 'G#3', '11.0', 'E4', 'G3', 'A3', 'G2', 'G#3', 'E4', 'F3', 'D4', 'E4', 'E3', 'D4', 'B3', 'A3', 'E3', 'D4', 'F4', 'A3', 'C2', 'E5', 'C3', 'G3', 'B2', 'F4', 'D4', 'B3', 'NULL', 'E3', 'E3', 'A3', 'G#3', 'E3', 'F3', 'NULL', 'NULL', 'B3', 'G3', 'NULL', 'NULL', 'A3', 'D4', 'G#3', 'G3', 'F3', 'F4', 'B3', 'C4', 'A3', 'C4', 'C4', 'D4', 'A3', 'C4', 'B-3', 'F4', 'NULL', 'G3', 'A3', 'A3', 'B3', 'E5', 'G3', 'A3', 'NULL', 'E3', 'B2', 'NULL', 'A3', 'C2', 'G3', 'NULL', 'E4', 'B2', 'A3', 'F3', 'A3', 'G5', 'NULL', 'A3', 'F4', 'B3', 'A3', 'A3', 'C4', 'A5', 'D4', 'NULL', 'A3', 'NULL', 'NULL', 'D4', 'G3', 'E4', 'B2', 'B5', 'NULL', 'B3', 'NULL', 'G#3', 'A3', 'E4', 'G#6', 'B3', 'G3', 'B3', 'NULL', 'F5', 'E4', 'A2', 'G5', 'A3', 'E3', 'NULL', 'A3', 'E3', 'B3', 'C3', 'NULL', 'B3', 'NULL', 'G3', 'A3', 'D4', 'E4', 'F3', 'G2', 'G#3', 'E3', 'G#3', 'G3', 'A3', 'G3', 'NULL', 'NULL', 'A3', 'A3', 'B3', 'NULL', 'E3', 'A3', 'B3', 'G3', 'D3', 'G2', 'NULL', 'NULL', 'A3', 'G3', 'E4']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 100, 50), dtype=float64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_output, patterns, next_index, pattern = generate_notes(model, x, temperature=1)\r\n",
    "print(prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, patterns, path):\r\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file from the notes\"\"\"\r\n",
    "    \r\n",
    "    offset = 0\r\n",
    "    output_notes = []\r\n",
    "\r\n",
    "    # create note and chord objects based on the values generated by the model\r\n",
    "    for pattern in prediction_output:\r\n",
    "        # pattern is a chord\r\n",
    "        if ('.' in pattern) or pattern.isdigit():\r\n",
    "            notes_in_chord = pattern.split('.')\r\n",
    "            notes = []\r\n",
    "            for current_note in notes_in_chord:\r\n",
    "                new_note = note.Note(int(current_note))\r\n",
    "                new_note.storedInstrument = instrument.Piano()\r\n",
    "                notes.append(new_note)\r\n",
    "            new_chord = chord.Chord(notes)\r\n",
    "            new_chord.offset = offset\r\n",
    "            output_notes.append(new_chord)\r\n",
    "        # pattern is a rest\r\n",
    "        elif (\"NULL\" in pattern):\r\n",
    "            new_rest = note.Rest(pattern)\r\n",
    "            output_notes.append(new_rest)\r\n",
    "        # pattern is a note\r\n",
    "        else:\r\n",
    "            new_note = note.Note(pattern)   \r\n",
    "            new_note.offset = offset\r\n",
    "            new_note.storedInstrument = instrument.Piano()\r\n",
    "            output_notes.append(new_note)\r\n",
    "\r\n",
    "        # increase offset each iteration so that notes do not stack\r\n",
    "        offset += 0.5\r\n",
    "\r\n",
    "    midi_stream = stream.Stream(output_notes)\r\n",
    "\r\n",
    "    midi_stream.write(\"midi\", fp= path + \"test_output_9.mid\")   # first output 01/07/2021\r\n",
    "\r\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi = create_midi(prediction_output, patterns, path_4)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_music(music_file):\r\n",
    "    \"\"\"\r\n",
    "    Play music given a midi file path\r\n",
    "    \"\"\"\r\n",
    "    try:\r\n",
    "        # allow to stop the piece \r\n",
    "        clock = pygame.time.Clock() \r\n",
    "        pygame.mixer.music.load(music_file)\r\n",
    "        pygame.mixer.music.play()\r\n",
    "        while pygame.mixer.music.get_busy():\r\n",
    "            # check if playback has finished\r\n",
    "            clock.tick(10)\r\n",
    "\r\n",
    "        freq = 44100    # audio CD quality\r\n",
    "        bitsize = -16   # unsigned 16 bit\r\n",
    "        channels = 2    # 1 is mono, 2 is stereo\r\n",
    "        buffer = 1024    # number of samples\r\n",
    "        pygame.mixer.init(freq, bitsize, channels, buffer)\r\n",
    "\r\n",
    "    except KeyboardInterrupt:\r\n",
    "        while True:\r\n",
    "            action = input('Enter Q to Quit, Enter to Skip. ').lower()\r\n",
    "            if action == 'q':\r\n",
    "                pygame.mixer.music.fadeout(1000)\r\n",
    "                pygame.mixer.music.stop()\r\n",
    "            else:\r\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_music(path_4 + \"test_output_9.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo de abajo aun no lo he utilizado para producir ningun midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_out(notes, pitchnames, n_vocab):\r\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\r\n",
    "    # map between notes and integers and back\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\r\n",
    "\r\n",
    "    sequence_length = 100\r\n",
    "    network_input = []\r\n",
    "    output = []\r\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\r\n",
    "        sequence_in = notes[i:i + sequence_length]\r\n",
    "        sequence_out = notes[i + sequence_length]\r\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\r\n",
    "        output.append(note_to_int[sequence_out])\r\n",
    "\r\n",
    "    n_patterns = len(network_input)\r\n",
    "\r\n",
    "    # reshape the input into a format compatible with LSTM layers\r\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\r\n",
    "    # normalize input\r\n",
    "    normalized_input = normalized_input / float(n_vocab)\r\n",
    "\r\n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no la uso aun - lo saque fuera de la funcion arriva\r\n",
    "def generate():\r\n",
    "    \"\"\" Generate a piano midi file \"\"\"\r\n",
    "\r\n",
    "    # Get all pitch names\r\n",
    "    pitchnames = sorted(set(item for item in notes))\r\n",
    "    # Get all pitch names\r\n",
    "    n_vocab = len(set(notes))\r\n",
    "\r\n",
    "    network_input, normalized_input = prepare_sequences_out(notes, pitchnames, n_vocab)\r\n",
    "    model = create_network()\r\n",
    "    prediction_output, pattern = generate_notes(model, network_input, n_vocab, loaded_notes)\r\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN\r\n",
    "# red neuronal que conoce los sonidos\r\n",
    "# red que predice tb recursiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read h5 format files \n",
    "import h5py\n",
    "filename = \"test.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8a740277f67c33143a8e5c8e55f738530a350d8def4a85d8635b690074994c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}