{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Soni - do**\r\n",
    "## **Generating Music with Machine Learning**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Sonia Cobo\n",
    "#### Date: July 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though this project doesn't have a hypothesis per se, it was done to kind off prove how AI has advanced and it is now able to generate music which has been associated with emotions and human capabilities for a long period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation - dividir canciones, modificarlas para tener mas datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input to the model will be a series of notes from a MIDI file. MIDI (Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors that connect a wide variety of electronic musical instruments and computers. They don't contain actual audio data and are small in size. They explain what notes are played, when they're played, and how long or loud each note should be.\r\n",
    "\r\n",
    "### To keep the project simple only files with one instrument were chosen, in this case the instrument is piano and the type of songs is classical. \r\n",
    "### These songs have been obtained from the following datasets: http://www.piano-midi.de/ and https://www.mfiles.co.uk/classical-midi.htm\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no descargardas aun: https://github.com/Skuldur/Classical-Piano-Composer/tree/master/midi_songs\n",
    "# https://drive.google.com/file/d/1qnQVK17DNVkU19MgVA4Vg88zRDvwCRXw/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\r\n",
    "import numpy as np\r\n",
    "import pandas as pd \r\n",
    "from random import randint\r\n",
    "from sqlalchemy import create_engine\r\n",
    "\r\n",
    "# manipulate midi files\r\n",
    "import glob\r\n",
    "from music21 import *\r\n",
    "#from music21 import converter, instrument, note, chord, meter, stream, duration, corpus\r\n",
    "import pygame\r\n",
    "\r\n",
    "# visualization\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# route files\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "# ml model\r\n",
    "import pickle\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "from keras.utils import np_utils\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Activation\r\n",
    "from keras.layers import BatchNormalization \r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "from keras.layers import Reshape\r\n",
    "from keras.layers import LeakyReLU\r\n",
    "from keras.layers import Conv2D\r\n",
    "from keras.layers import Conv1D\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import Conv2DTranspose\r\n",
    "from keras.layers import Conv1DTranspose\r\n",
    "\r\n",
    "# my libraries\r\n",
    "from utils.sql_tb import MySQL\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The route of this file is added to the sys path to be able to import/export functions\r\n",
    "sep = os.sep\r\n",
    "def route (steps):\r\n",
    "    \"\"\"\r\n",
    "    This function appends the route of the file to the sys path\r\n",
    "    to be able to import files from/to other foders within the EDA project folder.\r\n",
    "    \"\"\"\r\n",
    "    route = os.getcwd()\r\n",
    "    for i in range(steps): \r\n",
    "        route = os.path.dirname(route)\r\n",
    "    sys.path.append(route)\r\n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\r\n",
    "\r\n",
    "# path to raw data\r\n",
    "path = route(1) + sep + \"data\" + sep + \"raw_data\" + sep\r\n",
    "# path to data in the right key\r\n",
    "path_1 = route(1) + sep + \"data\" + sep + \"converted_data\" + sep\r\n",
    "# path to compiled notes list\r\n",
    "path_2 = route(1) + sep + \"data\" + sep + \"notes\" + sep\r\n",
    "# path to generated models\r\n",
    "path_3 = route(1) + sep + \"models\" + sep\r\n",
    "# path to generated midi files\r\n",
    "path_4 = route(1) + sep + \"reports\" + sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midi file exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All information from the midi file (i.e. notes, pitch, chord, time signature, etc) is contained within the component list\r\n",
    "\r\n",
    "def info_midi (path, filename):\r\n",
    "    \"\"\"\r\n",
    "    It returns all midi file information given its path and filename\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    # Convert to Score object\r\n",
    "    file = converter.parse(path + filename)\r\n",
    "    components = []\r\n",
    "    # read file information\r\n",
    "    for element in file.recurse():  \r\n",
    "        components.append(element)\r\n",
    "    return components\r\n",
    "\r\n",
    "components = info_midi(path, \"alb_esp1.mid\")\r\n",
    "#components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each midi file contains notes and chords. These two properties will be the input and output of the LSTM network so \r\n",
    "# they need to be taken out from all midi files. \r\n",
    "\r\n",
    "def get_notes_per_song(path, filename, save_path, save_name):\r\n",
    "    \"\"\"\r\n",
    "    This function extracts all the notes, rests and chords from one midi file\r\n",
    "    and saves it in a list in the converted_data folder.\r\n",
    "\r\n",
    "    Param: Path of the midi file, filename (str)\r\n",
    "    \"\"\"\r\n",
    "    components = info_midi(path, filename)\r\n",
    "    note_list = []\r\n",
    "    \r\n",
    "    for element in components:\r\n",
    "        # note pitches are extracted\r\n",
    "        if isinstance(element, note.Note):\r\n",
    "            note_list.append(str(element.pitch))\r\n",
    "        # chords are extracted\r\n",
    "        elif isinstance(element, chord.Chord):\r\n",
    "            note_list.append(\".\".join(str(n) for n in element.normalOrder))    \r\n",
    "        # rests are extracted\r\n",
    "        elif isinstance(element, note.Rest):\r\n",
    "            note_list.append(\"NULL\")    #further transformation needs this value as str rather than np.nan\r\n",
    "\r\n",
    "    # save list with all componenets extracted\r\n",
    "    with open(save_path + save_name, \"wb\") as filepath:\r\n",
    "        pickle.dump(note_list, filepath)\r\n",
    "    \r\n",
    "    return note_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_song = get_notes_per_song(path_1, \"C_alb_esp1.mid\", path_2, \"one_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_notes(path, save_name, save_path):\r\n",
    "    \"\"\"\r\n",
    "    This function extracts all the notes, rests and chords from all midi files \r\n",
    "    and saves it in a list in the converted_data folder.\r\n",
    "\r\n",
    "    Param: Path of the midi file     \r\n",
    "    \"\"\"\r\n",
    "    all_notes = []\r\n",
    "    list_path = os.listdir(path)\r\n",
    "    for filename in list_path:\r\n",
    "        output = get_notes_per_song(path, filename, save_path, save_name)\r\n",
    "        all_notes += output\r\n",
    "        \r\n",
    "    return all_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes = get_all_notes(path = path_1, save_path = path_2, save_name = \"all_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load notes and chords previously separated\r\n",
    "def load_notes (path, filename):\r\n",
    "    \"\"\"\r\n",
    "    Load the note list containing pitches, rests and chords.\r\n",
    "    \r\n",
    "    Param: Path of the saved note list, and its name as string\r\n",
    "    \"\"\"\r\n",
    "    with open(path + filename, \"rb\") as f:\r\n",
    "        loaded_notes = pickle.load(f)\r\n",
    "        return loaded_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_chopin = load_notes(path_2, \"notes_chopin\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_chopin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, min_note_occurence, sequence_length, step):\r\n",
    "    \"\"\" \r\n",
    "    This function creates the input and output sequences used by the neural network.\r\n",
    "    It returns the x and y of the model.\r\n",
    "\r\n",
    "    Param: \r\n",
    "        Note: List containing all notes, rests and chords\r\n",
    "        Sequence_length: Lenght of notes given to the model to help predict the next\r\n",
    "        Step: Step (int) between one input sequence and the next one\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # get all pitchnames\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    print('Total unique notes:', len(pitchnames))\r\n",
    "\r\n",
    "    # Calculate occurence\r\n",
    "    note_freq = {}\r\n",
    "    for elem in notes:\r\n",
    "        note_freq[elem] = note_freq.get(elem, 0) + 1\r\n",
    "\r\n",
    "    ignored_notes = set()\r\n",
    "    for k, v in note_freq.items():\r\n",
    "        if note_freq[k] < min_note_occurence:\r\n",
    "            ignored_notes.add(k)\r\n",
    "    \r\n",
    "    \r\n",
    "    print('Unique words before ignoring:', len(pitchnames))\r\n",
    "    print('Ignoring words with occurence <', min_note_occurence)\r\n",
    "    pitchnames = sorted(set(pitchnames) - ignored_notes)\r\n",
    "    print('Unique words after ignoring:', len(pitchnames))\r\n",
    "\r\n",
    "    # create a dictionary to convert pitches (strings) to integers\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))  # rests are included  \r\n",
    "\r\n",
    "    network_input = []\r\n",
    "    network_output = []\r\n",
    "\r\n",
    "    # create input sequences and the corresponding outputs\r\n",
    "    for i in range(0, len(notes) - sequence_length, step): \r\n",
    "        # remove ignored notes from the note list   \r\n",
    "        if len(set(notes[i: i+ sequence_length + 1]).intersection(ignored_notes)) == 0:\r\n",
    "            network_input.append(notes[i:i + sequence_length])\r\n",
    "            network_output.append(notes[i + sequence_length])\r\n",
    "    # array of zeros\r\n",
    "    x = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    y = np.zeros((len(network_input), len(pitchnames)))\r\n",
    "    # exchange note values for their integer-code\r\n",
    "    for i, sequence in enumerate(network_input):\r\n",
    "        for j, note in enumerate(sequence):\r\n",
    "            x[i, j, note_to_int[note]] = 1\r\n",
    "        y[i, note_to_int[network_output[i]]] = 1\r\n",
    "\r\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique notes: 109\n",
      "Unique words before ignoring: 109\n",
      "Ignoring words with occurence < 1\n",
      "Unique words after ignoring: 109\n"
     ]
    }
   ],
   "source": [
    "x, y = prepare_sequences(notes=load_chopin, min_note_occurence=1, sequence_length=100, step=3)  # length y step pueden variar  \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474, 100, 109)\n",
      "(474, 109)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\r\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_gan(notes, sequence_length, step):\r\n",
    "    \"\"\" \r\n",
    "    Prepare the sequences used by the neural network \r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # get all pitchnames\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    print('Total unique notes:', len(pitchnames))\r\n",
    "\r\n",
    "    # create a dictionary to convert pitches (strings) to integers\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))  # rests are included  \r\n",
    "\r\n",
    "    network_input = []\r\n",
    "    network_output = []\r\n",
    "\r\n",
    "    #sequence_in = []\r\n",
    "    #sequence_out = []\r\n",
    "\r\n",
    "    # create input sequences and the corresponding outputs\r\n",
    "    for i in range(0, len(notes) - 2*sequence_length, step):    \r\n",
    "        network_input.append(notes[i:i + sequence_length])\r\n",
    "        network_output.append(notes[i + sequence_length : i + 2*sequence_length])\r\n",
    "        # exchange their values for their integer-code\r\n",
    "\r\n",
    "        # network_input.append([note_to_int[elem] for elem in sequence_in])\r\n",
    "        # network_output.append(note_to_int[sequence_out])\r\n",
    "\r\n",
    "    x = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    y = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    for i, sequence in enumerate(network_input):\r\n",
    "        for j, note in enumerate(sequence):\r\n",
    "            x[i, j, note_to_int[note]] = 1\r\n",
    "            y[i, j, note_to_int[network_output[i][j]]] = 1\r\n",
    "\r\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38737"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique notes: 109\n"
     ]
    }
   ],
   "source": [
    "x_gan, y_gan = prepare_sequences_gan(notes=load_chopin, sequence_length=100, step=3)  # length y step pueden variar  \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474, 100, 109)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.shape[0], x.shape[1], x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(n_nodes= x.shape[0] , latent_dim=(x.shape[1], x.shape[2])):\r\n",
    "\tmodel = Sequential()\r\n",
    "\tmodel.add(Dense(n_nodes, input_shape = latent_dim))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\tmodel.add(Dense(n_nodes))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\tmodel.add(BatchNormalization())\r\n",
    "\tmodel.add(Dense(x.shape[2]))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\tmodel.add(BatchNormalization())\r\n",
    "\tmodel.add(Reshape(latent_dim))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\tmodel.add(Conv1DTranspose(n_nodes, 3, padding='same'))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\tmodel.add(Conv1DTranspose(x.shape[2], 3, activation='softmax', padding='same'))\t# softmax - n elem = n salidas\r\n",
    "\t\r\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_83 (Dense)             (None, 100, 474)          52140     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_163 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100, 474)          225150    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_164 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 100, 474)          1896      \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 100, 109)          51775     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_165 (LeakyReLU)  (None, 100, 109)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 100, 109)          436       \n",
      "_________________________________________________________________\n",
      "reshape_28 (Reshape)         (None, 100, 109)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_166 (LeakyReLU)  (None, 100, 109)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_15 (Conv1DT (None, 100, 474)          155472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_167 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_16 (Conv1DT (None, 100, 109)          155107    \n",
      "=================================================================\n",
      "Total params: 641,976\n",
      "Trainable params: 640,810\n",
      "Non-trainable params: 1,166\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g = generator_model(n_nodes= x.shape[0] , latent_dim=(x.shape[1],x.shape[2]))\r\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\r\n",
    "    # helper function to sample an index from a probability array\r\n",
    "    preds = np.asarray(preds).astype(\"float64\")\r\n",
    "    preds = np.log(preds) / temperature\r\n",
    "    exp_preds = np.exp(preds)\r\n",
    "    preds = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, preds, 1)\r\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes_gan(notes, model, temperature=1.0):\r\n",
    "    \"\"\" \r\n",
    "    Generate notes from the neural network based on a sequence of notes \r\n",
    "    \"\"\"\r\n",
    "    # pick a random sequence from the input as a starting point for the prediction\r\n",
    "    start = np.random.randint(0, len(notes)-100-1)\r\n",
    "\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \r\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\r\n",
    "    pattern = notes[start: (start+100)] \r\n",
    "    prediction_output = []\r\n",
    "    patterns = []\r\n",
    "\r\n",
    "    # generate 500 notes, roughly two minutes of music\r\n",
    "\r\n",
    "    prediction_input = np.zeros((1, 100, len(pitchnames)))\r\n",
    "    for j, note in enumerate(pattern):\r\n",
    "        prediction_input[0, j, note_to_int[note]] = 1.0\r\n",
    "    preds = model.predict(prediction_input, verbose=0)[0]\r\n",
    "    for elem in list(preds):\r\n",
    "        next_index = sample(elem, temperature=temperature)\r\n",
    "        next_note = int_to_note[next_index]\r\n",
    "        #pattern = pattern[1:]\r\n",
    "        #pattern.append(next_note)\r\n",
    "        prediction_output.append(next_note)\r\n",
    "\r\n",
    "        patterns.append(next_index)\r\n",
    "        #patterns = patterns[1:len(patterns)]\r\n",
    "    return prediction_output, patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8.11.2.4', 'E-3', '10.2', 'B3', 'C#3', 'A4', '9.2', 'E6', 'G#4', '10.2', 'E-6', '9.0.3', '8.11.2.4', 'C#3', '4', 'B-5', 'C#2', '4.8', '10.1', '2.5', 'G5', 'E-3', 'C#6', 'G4', 'G2', 'A6', 'D4', 'C3', 'G#2', '6.9.1', '2.7', 'A2', '9', '2.6', '9.0.3', '8.11', '1.2', 'D4', '1.6', '5.9.11', '1.6', 'F#3', '9.0.4', '9.11', 'G#5', '1.6', 'E6', 'B-5', '9.0.4', '9.11', '2.7', '9', '8.11', '6.9.1', '2.6', '6.11', '4.7.11', '11.4', '2.5', 'B-4', '9.0', '10.0', 'B-4', 'E1', 'A5', '11.3', 'G#2', '9.0.3', 'E-5', 'F#3', '4.6.10', 'G4', '4.7.10.0', 'C#4', 'B4', '7.0', '1.4', 'A6', '2.4.6.10', 'C#3', 'D3', 'E-4', 'C3', 'C6', 'C#2', '1.6', '4.6.10', 'E3', 'B-4', '10.2', '3.9', 'F4', 'A6', '0.3.6', '2.8', 'E-6', 'C#2', 'G#3', '6.9', 'F#5']\n"
     ]
    }
   ],
   "source": [
    "prediction_output, patterns = generate_notes_gan(load_chopin, g, temperature=1)\r\n",
    "print(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, patterns, path):\r\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file from the notes\"\"\"\r\n",
    "    \r\n",
    "    offset = 0\r\n",
    "    output_notes = []\r\n",
    "\r\n",
    "    # create note and chord objects based on the values generated by the model\r\n",
    "    for pattern in prediction_output:\r\n",
    "        # pattern is a chord\r\n",
    "        if ('.' in pattern) or pattern.isdigit():\r\n",
    "            notes_in_chord = pattern.split('.')\r\n",
    "            notes = []\r\n",
    "            for current_note in notes_in_chord:\r\n",
    "                new_note = note.Note(int(current_note))\r\n",
    "                new_note.storedInstrument = instrument.Piano()\r\n",
    "                notes.append(new_note)\r\n",
    "            new_chord = chord.Chord(notes)\r\n",
    "            new_chord.offset = offset\r\n",
    "            output_notes.append(new_chord)\r\n",
    "        # pattern is a rest\r\n",
    "        elif (\"NULL\" in pattern):\r\n",
    "            new_rest = note.Rest(pattern)\r\n",
    "            output_notes.append(new_rest)\r\n",
    "        # pattern is a note\r\n",
    "        else:\r\n",
    "            new_note = note.Note(pattern)   \r\n",
    "            new_note.offset = offset\r\n",
    "            new_note.storedInstrument = instrument.Piano()\r\n",
    "            output_notes.append(new_note)\r\n",
    "\r\n",
    "        # increase offset each iteration so that notes do not stack\r\n",
    "        offset += 0.5\r\n",
    "\r\n",
    "    midi_stream = stream.Stream(output_notes)\r\n",
    "\r\n",
    "    midi_stream.write(\"midi\", fp= path + \"test_output_11_gmodel_chopin.mid\")   # first output 01/07/2021\r\n",
    "\r\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi = create_midi(prediction_output, patterns, path_4)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_music(music_file):\r\n",
    "    \"\"\"\r\n",
    "    Play music given a midi file path\r\n",
    "    \"\"\"\r\n",
    "    import music21\r\n",
    "    try:\r\n",
    "        # allow to stop the piece \r\n",
    "        pygame.mixer.init()\r\n",
    "        clock = pygame.time.Clock() \r\n",
    "        pygame.mixer.music.load(music_file)\r\n",
    "        pygame.mixer.music.play()\r\n",
    "        while pygame.mixer.music.get_busy():\r\n",
    "            # check if playback has finished\r\n",
    "            clock.tick(10)\r\n",
    "\r\n",
    "        freq = 44100    # audio CD quality\r\n",
    "        bitsize = -16   # unsigned 16 bit\r\n",
    "        channels = 2    # 1 is mono, 2 is stereo\r\n",
    "        buffer = 1024    # number of samples\r\n",
    "        pygame.mixer.init(freq, bitsize, channels, buffer)\r\n",
    "\r\n",
    "    except KeyboardInterrupt:\r\n",
    "        while True:\r\n",
    "            action = input('Enter Q to Quit, Enter to Skip.').lower()\r\n",
    "            if action == 'q':\r\n",
    "                pygame.mixer.music.fadeout(1000)\r\n",
    "                pygame.mixer.music.stop()\r\n",
    "            else:\r\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plays music when the cell is executed \r\n",
    "\r\n",
    "play_music(path_4 + \"test_output_11_gmodel_chopin.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasta aquÃ­ va!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(x, n_samples):\r\n",
    "    \"\"\"\r\n",
    "    Load and prepare training notes\r\n",
    "    \"\"\"\r\n",
    "    # choose random instances\r\n",
    "    start = np.random.randint(0, len(x)-100-1)\r\n",
    "    # retrieve selected images\r\n",
    "    x_real = x[start: (start+n_samples)] \r\n",
    "    # generate 'real' class labels (1)\r\n",
    "    y_real = np.ones((n_samples, 1))\r\n",
    "\r\n",
    "    return x_real, y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real, y_real = generate_real_samples(x, n_samples=100)\r\n",
    "y_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(x, n_samples):\r\n",
    "    import random\r\n",
    "    # create random matrix of numbers \r\n",
    "    x_latent = np.zeros((n_samples, x.shape[1], x.shape[2]))\r\n",
    "  \r\n",
    "    for j, elem in enumerate(x_latent):\r\n",
    "        for k, row in enumerate(elem):\r\n",
    "            num = random.randint(0, x.shape[2])\r\n",
    "            for i in range(len(row)):\r\n",
    "                if i == num:\r\n",
    "                    x_latent[j][k][i] = 1\r\n",
    "\r\n",
    "    return x_latent\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_latent = generate_latent_points(x, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_data(x, g_model, n_samples):\r\n",
    "\t# create 'fake' class labels (0)\r\n",
    "\ty_fake = np.zeros((n_samples, 1))\r\n",
    "\r\n",
    "\t# generate points in latent space\r\n",
    "\tx_latent = generate_latent_points(x, n_samples)\r\n",
    "\t# predict outputs\r\n",
    "\tx_fake = g_model.predict(x_latent, verbose=0)\r\n",
    "\r\n",
    "\treturn x_fake, y_fake\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fake, y_fake = generate_fake_data(x, g, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\r\n",
    "def discriminator_model(num_units, n_inputs=(x.shape[1], x.shape[2])):\r\n",
    "\tmodel = Sequential()\r\n",
    "\t# normal\r\n",
    "\tmodel.add(Conv1D(num_units, (3), padding='same', input_shape=n_inputs))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\t# downsample to 40x40\r\n",
    "\tmodel.add(Conv1D(num_units, (3), padding='same'))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\t# downsample to 20x30\r\n",
    "\tmodel.add(Conv1D(num_units, (3), padding='same'))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\tmodel.add(BatchNormalization())\r\n",
    "\t# downsample to 10x10\r\n",
    "\tmodel.add(Conv1D(num_units, (3), padding='same'))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\t# downsample to 5x5\r\n",
    "\tmodel.add(Conv1D(num_units, (3), padding='same'))\r\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\r\n",
    "\t# classifier\r\n",
    "\tmodel.add(Flatten())\r\n",
    "\tmodel.add(Dropout(0.4))\r\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
    "\t# compile model\r\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\r\n",
    "\treturn model\r\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_39 (Conv1D)           (None, 100, 474)          155472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_168 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 100, 474)          674502    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_169 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 100, 474)          674502    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_170 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 100, 474)          1896      \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 100, 474)          674502    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_171 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 100, 474)          674502    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_172 (LeakyReLU)  (None, 100, 474)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 47400)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 47400)             0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 47401     \n",
      "=================================================================\n",
      "Total params: 2,902,777\n",
      "Trainable params: 2,901,829\n",
      "Non-trainable params: 948\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d = discriminator_model(num_units=x.shape[0])\r\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_model(g_model, d_model):\r\n",
    "\t# make weights in the discriminator not trainable\r\n",
    "\td_model.trainable = False\r\n",
    "\t# connect them\r\n",
    "\tmodel = Sequential()\r\n",
    "\t# add generator\r\n",
    "\tmodel.add(g_model)\r\n",
    "\tmodel.add(BatchNormalization())\r\n",
    "\t# add the discriminator\r\n",
    "\tmodel.add(d_model)\r\n",
    "\t# compile model\r\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=\"adam\")\r\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_67 (Sequential)   (None, 100, 109)          641976    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 100, 109)          436       \n",
      "_________________________________________________________________\n",
      "sequential_68 (Sequential)   (None, 1)                 2902777   \n",
      "=================================================================\n",
      "Total params: 3,545,189\n",
      "Trainable params: 641,028\n",
      "Non-trainable params: 2,904,161\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_model = gan_model(g, d)\r\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the discriminator model is updated for a half batch of real samples, then a half batch of fake samples, \r\n",
    "# together forming one batch of weight updates. The generator is then updated via the combined GAN model. \r\n",
    "# Importantly, the class label is set to 1 or real for the fake samples. This has the effect of updating the generator toward \r\n",
    "# getting better at generating real samples on the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator, plot generated images, save generator model\r\n",
    "def check_performance(epoch, g_model, d_model, x, n_samples=100):\r\n",
    "\t# prepare real samples\r\n",
    "\tx_real, y_real = generate_real_samples(x, n_samples)\r\n",
    "\t# evaluate discriminator on real examples\r\n",
    "\t_, acc_real = d_model.evaluate(x_real, y_real, verbose=0)\r\n",
    "\t# prepare fake examples\r\n",
    "\tx_fake, y_fake = generate_fake_data(x, g, n_samples)\r\n",
    "\t# evaluate discriminator on fake examples\r\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\r\n",
    "\t# summarize discriminator performance\r\n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\r\n",
    "\t# save plot\r\n",
    "\t#save_plot(x_fake, epoch)\r\n",
    "\t# save the generator model tile file\r\n",
    "\t#filename = 'generator_model_%03d.h5' % (epoch+1)\r\n",
    "\t#g_model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Accuracy real: 40%, fake: 100%\n"
     ]
    }
   ],
   "source": [
    "check_performance = check_performance(1, g, d, x, n_samples=100)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\r\n",
    "def train(x, g_model, d_model, gan_model, y_gan, n_epochs, n_batch):\r\n",
    "\tbatch_per_epoch = int(x.shape[0]/n_batch)\r\n",
    "\thalf_batch = int(n_batch / 2)\r\n",
    "\t# manually enumerate epochs\r\n",
    "\tfor i in range(n_epochs):\r\n",
    "\t\t# enumerate batches over the training set\r\n",
    "\t\tfor j in range(batch_per_epoch):\r\n",
    "\t\t\t# get randomly selected 'real' samples\r\n",
    "\t\t\tx_real, y_real = generate_real_samples(x, n_samples=half_batch)\r\n",
    "\t\t\t# update discriminator model weights\r\n",
    "\t\t\td_loss1, _ = d_model.train_on_batch(x_real, y_real)\r\n",
    "\t\t\t# generate 'fake' examples \r\n",
    "\t\t\tx_fake, y_fake = generate_fake_data(x, g, n_samples=half_batch)\r\n",
    "\t\t\t# update discriminator model weights\r\n",
    "\t\t\td_loss2, _ = d_model.train_on_batch(x_fake, y_fake)\r\n",
    "\t\t\t# prepare points in latent space as input for the generator\r\n",
    "\t\t\tx_latent = generate_latent_points(x, n_samples=n_batch)\r\n",
    "\t\t\t# create inverted labels for the fake samples\r\n",
    "\t\t\ty_fake_1 = np.ones((n_batch, 1))\r\n",
    "\t\t\t# update the generator via the discriminator's error\r\n",
    "\t\t\tg_loss = gan_model.train_on_batch(x_latent, y_fake_1)\r\n",
    "\t\t\t# summarize loss on this batch\r\n",
    "\t\t\tprint('>%d, %d/%d, loss_real=%.3f, loss_fake=%.3f loss_latent=%.3f' %\r\n",
    "\t\t\t\t(i+1, j+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\r\n",
    "\t\t# evaluate the model performance, sometimes\r\n",
    "\t\tif (n_epochs+1) % 10 == 0:\r\n",
    "\t\t\tcheck_performance(n_epochs, g_model, d_model, x, n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/3, loss_real=0.828, loss_fake=0.762 loss_latent=0.388\n",
      ">1, 2/3, loss_real=0.000, loss_fake=0.696 loss_latent=0.158\n",
      ">1, 3/3, loss_real=0.000, loss_fake=0.627 loss_latent=0.027\n",
      ">2, 1/3, loss_real=0.000, loss_fake=0.548 loss_latent=0.002\n",
      ">2, 2/3, loss_real=0.000, loss_fake=0.410 loss_latent=0.000\n",
      ">2, 3/3, loss_real=0.000, loss_fake=0.184 loss_latent=0.000\n",
      ">3, 1/3, loss_real=0.000, loss_fake=0.017 loss_latent=0.000\n",
      ">3, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.001\n",
      ">3, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.005\n",
      ">4, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.027\n",
      ">4, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.045\n",
      ">4, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.006\n",
      ">5, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">5, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">5, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">6, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">6, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">6, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">7, 1/3, loss_real=0.551, loss_fake=0.000 loss_latent=0.000\n",
      ">7, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">7, 3/3, loss_real=0.000, loss_fake=23.283 loss_latent=0.000\n",
      ">8, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">8, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.032\n",
      ">8, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.042\n",
      ">9, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.002\n",
      ">9, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">9, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">10, 1/3, loss_real=0.000, loss_fake=0.005 loss_latent=0.141\n",
      ">10, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.022\n",
      ">10, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.235\n",
      ">11, 1/3, loss_real=0.202, loss_fake=0.184 loss_latent=0.000\n",
      ">11, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">11, 3/3, loss_real=0.008, loss_fake=0.000 loss_latent=0.000\n",
      ">12, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">12, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">12, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">13, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">13, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">13, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">14, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">14, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">14, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">15, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">15, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">15, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">16, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">16, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">16, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">17, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">17, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">17, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">18, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">18, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">18, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">19, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">19, 2/3, loss_real=1.337, loss_fake=0.000 loss_latent=0.000\n",
      ">19, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">20, 1/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">20, 2/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">20, 3/3, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n"
     ]
    }
   ],
   "source": [
    "train(x=x, g_model=g, d_model=d, gan_model=gan_model, y_gan=y_gan, n_epochs=20, n_batch=129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# save the model\r\n",
    "g.save(path_3 + \"model_g_1con.h5\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load the model \r\n",
    "# model_g_4 - gan con lstm 3 - 100 epochs, 139 batches\r\n",
    "# model_g_1con - gan con con 3 - 100 epochs, 139 batches\r\n",
    "model_gan_1con = tf.keras.models.load_model(path_3 + \"model_g_1con.h5\") # 3 - 100 epochs, 139 batches\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#gan_model_1 = tf.keras.models.load_model(path_3 + \"model_g_3.h5\") \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(notes, model, temperature=1.0):\r\n",
    "    \"\"\" \r\n",
    "    Generate notes from the neural network based on a sequence of notes \r\n",
    "    \"\"\"\r\n",
    "    # pick a random sequence from the input as a starting point for the prediction\r\n",
    "    start = np.random.randint(0, len(notes)-100-1)\r\n",
    "\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \r\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\r\n",
    "    \r\n",
    "    pattern = notes[start: (start+100)] \r\n",
    "    prediction_output = []\r\n",
    "    patterns = []\r\n",
    "\r\n",
    "    # generate 500 notes, roughly two minutes of music\r\n",
    "    for note_index in range(100):\r\n",
    "        prediction_input = np.zeros((1, 100, len(pitchnames)))\r\n",
    "        for j, note in enumerate(pattern):\r\n",
    "            prediction_input[0, j, note_to_int[note]] = 1.0\r\n",
    "        preds = model.predict(prediction_input, verbose=0)[0]\r\n",
    "        for elem in list(preds):\r\n",
    "            next_index = sample(elem, temperature=temperature)\r\n",
    "            next_note = int_to_note[next_index]\r\n",
    "\r\n",
    "        pattern = pattern[1:]\r\n",
    "        pattern.append(next_note)\r\n",
    "\r\n",
    "        prediction_output.append(next_note)\r\n",
    "\r\n",
    "        patterns.append(next_index)\r\n",
    "        #patterns = patterns[1:len(patterns)]\r\n",
    "\r\n",
    "    return prediction_output, patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D6', '2.6', 'E-5', '9.0.4', '1.5', 'F#5', '1.7', '2.4.8', '5.9.11', '4.7.9', '2.6', '6.9', '5.11', '11.2', '2.4.6.10', '9.11', 'C2', '9.11', 'B1', '10.0', '5.10', 'D3', 'G2', 'F4', '5.9.11', 'C2', '8.11.2', 'E4', '9', '5.7', 'C#4', '4.8', 'C3', 'E5', '9.0', '1.2', 'E6', 'C6', 'G#2', '0.4.6', 'E-5', 'B5', 'G2', 'A4', 'C6', 'D4', 'E3', 'B2', 'G#3', 'B-4', '4.7.10.0', 'A2', 'C3', '9.0.4', '0.4.6', 'E-4', 'A4', 'C#6', 'F5', 'D5', '2.7', '8.9', '8.11.2.4', 'A2', 'E1', '10.0', '9.0.3', 'NULL', '9.0.3', 'C4', '3.9', 'E-4', 'E-3', '10.2', '5.10', '9', '8.9', '9', '5.9.11', 'G#2', '11.3', 'G#4', 'E3', 'B5', 'E-5', 'E-3', '3.9', '11.2.6', '1.6', '5.9.11', '2.4.6.10', '1.7', 'C#4', 'E1', 'G2', '9', 'G2', '4.7.10.0', '9.0', '1.5']\n"
     ]
    }
   ],
   "source": [
    "prediction_output, patterns = generate_notes(load_chopin, model_gan_1con, temperature=1)\r\n",
    "print(prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, patterns, path):\r\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file from the notes\"\"\"\r\n",
    "    \r\n",
    "    offset = 0\r\n",
    "    output_notes = []\r\n",
    "\r\n",
    "    # create note and chord objects based on the values generated by the model\r\n",
    "    for pattern in prediction_output:\r\n",
    "        # pattern is a chord\r\n",
    "        if ('.' in pattern) or pattern.isdigit():\r\n",
    "            notes_in_chord = pattern.split('.')\r\n",
    "            notes = []\r\n",
    "            for current_note in notes_in_chord:\r\n",
    "                new_note = note.Note(int(current_note))\r\n",
    "                new_note.storedInstrument = instrument.Piano()\r\n",
    "                notes.append(new_note)\r\n",
    "            new_chord = chord.Chord(notes)\r\n",
    "            new_chord.offset = offset\r\n",
    "            output_notes.append(new_chord)\r\n",
    "        # pattern is a rest\r\n",
    "        elif (\"NULL\" in pattern):\r\n",
    "            new_rest = note.Rest(pattern)\r\n",
    "            output_notes.append(new_rest)\r\n",
    "        # pattern is a note\r\n",
    "        else:\r\n",
    "            new_note = note.Note(pattern)   \r\n",
    "            new_note.offset = offset\r\n",
    "            new_note.storedInstrument = instrument.Piano()\r\n",
    "            output_notes.append(new_note)\r\n",
    "\r\n",
    "        # increase offset each iteration so that notes do not stack\r\n",
    "        offset += 0.5\r\n",
    "\r\n",
    "    midi_stream = stream.Stream(output_notes)\r\n",
    "\r\n",
    "    midi_stream.write(\"midi\", fp= path + \"test_output_1ganconv.mid\")   # first output 01/07/2021\r\n",
    "\r\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi = create_midi(prediction_output, patterns, path_4)\r\n",
    "# 13gen - 100 epoch, 129 batches - lstm\r\n",
    "# test_output_1ganconv - 100 epoch, 129 batch - conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_music(music_file):\r\n",
    "    \"\"\"\r\n",
    "    Play music given a midi file path\r\n",
    "    \"\"\"\r\n",
    "    import music21\r\n",
    "    try:\r\n",
    "        # allow to stop the piece \r\n",
    "        pygame.mixer.init()\r\n",
    "        clock = pygame.time.Clock() \r\n",
    "        pygame.mixer.music.load(music_file)\r\n",
    "        pygame.mixer.music.play()\r\n",
    "        while pygame.mixer.music.get_busy():\r\n",
    "            # check if playback has finished\r\n",
    "            clock.tick(10)\r\n",
    "\r\n",
    "        freq = 44100    # audio CD quality\r\n",
    "        bitsize = -16   # unsigned 16 bit\r\n",
    "        channels = 2    # 1 is mono, 2 is stereo\r\n",
    "        buffer = 1024    # number of samples\r\n",
    "        pygame.mixer.init(freq, bitsize, channels, buffer)\r\n",
    "\r\n",
    "    except KeyboardInterrupt:\r\n",
    "        while True:\r\n",
    "            action = input('Enter Q to Quit, Enter to Skip.').lower()\r\n",
    "            if action == 'q':\r\n",
    "                pygame.mixer.music.fadeout(1000)\r\n",
    "                pygame.mixer.music.stop()\r\n",
    "            else:\r\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plays music when the cell is executed \r\n",
    "\r\n",
    "play_music(path_4 + \"test_output_1ganconv.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8a740277f67c33143a8e5c8e55f738530a350d8def4a85d8635b690074994c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}