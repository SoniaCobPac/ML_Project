{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Soni - do**\r\n",
    "## **Generating Music with Machine Learning**\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Author: Sonia Cobo\n",
    "#### Date: July 2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Though this project doesn't have a hypothesis per se, it was done to kind off prove how AI has advanced and it is now able to generate music which has been associated with emotions and human capabilities for a long period of time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# data augmentation - dividir canciones, modificarlas para tener mas datos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The input to the model will be a series of notes from a MIDI file. MIDI (Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors that connect a wide variety of electronic musical instruments and computers. They don't contain actual audio data and are small in size. They explain what notes are played, when they're played, and how long or loud each note should be.\r\n",
    "\r\n",
    "### To keep the project simple only files with one instrument were chosen, in this case the instrument is piano and the type of songs is classical. \r\n",
    "### These songs have been obtained from the following datasets: http://www.piano-midi.de/ and https://www.mfiles.co.uk/classical-midi.htm\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# no descargardas aun: https://github.com/Skuldur/Classical-Piano-Composer/tree/master/midi_songs\n",
    "# https://drive.google.com/file/d/1qnQVK17DNVkU19MgVA4Vg88zRDvwCRXw/view"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import all libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# data manipulation\r\n",
    "import numpy as np\r\n",
    "import pandas as pd \r\n",
    "from random import randint\r\n",
    "from sqlalchemy import create_engine\r\n",
    "\r\n",
    "# manipulate midi files\r\n",
    "import glob\r\n",
    "from music21 import *\r\n",
    "#from music21 import converter, instrument, note, chord, meter, stream, duration, corpus\r\n",
    "import pygame\r\n",
    "\r\n",
    "# visualization\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# route files\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "# ml model\r\n",
    "import pickle\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "from keras.utils import np_utils\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Activation\r\n",
    "from keras.layers import BatchNormalization \r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "from keras.layers import Reshape\r\n",
    "\r\n",
    "# my libraries\r\n",
    "from utils.sql_tb import MySQL\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from tensorflow.python.client import device_lib\r\n",
    "print(device_lib.list_local_devices())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7525799484492965909\n",
      "]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# The route of this file is added to the sys path to be able to import/export functions\n",
    "sep = os.sep\n",
    "def route (steps):\n",
    "    \"\"\"\n",
    "    This function appends the route of the file to the sys path\n",
    "    to be able to import files from/to other foders within the EDA project folder.\n",
    "    \"\"\"\n",
    "    route = os.getcwd()\n",
    "    for i in range(steps):\n",
    "        route = os.path.dirname(route)\n",
    "    sys.path.append(route)\n",
    "    return route"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# paths\r\n",
    "\r\n",
    "# path to raw data\r\n",
    "path = route(1) + sep + \"data\" + sep + \"raw_data\" + sep\r\n",
    "# path to data in the right key\r\n",
    "path_1 = route(1) + sep + \"data\" + sep + \"converted_data\" + sep\r\n",
    "# path to compiled notes list\r\n",
    "path_2 = route(1) + sep + \"data\" + sep + \"notes\" + sep\r\n",
    "# path to generated models\r\n",
    "path_3 = route(1) + sep + \"models\" + sep\r\n",
    "# path to generated midi files\r\n",
    "path_4 = route(1) + sep + \"reports\" + sep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Midi file exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hablar de frecuencia y la transpuesta de fourier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# All information from the midi file (i.e. notes, pitch, chord, time signature, etc) is contained within the component list\r\n",
    "\r\n",
    "def info_midi (path, filename):\r\n",
    "    \"\"\"\r\n",
    "    It returns all midi file information given its path and filename\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    # Convert to Score object\r\n",
    "    file = converter.parse(path + filename)\r\n",
    "    components = []\r\n",
    "    # read file information\r\n",
    "    for element in file.recurse():  \r\n",
    "        components.append(element)\r\n",
    "    return components\r\n",
    "\r\n",
    "components = info_midi(path, \"alb_esp1.mid\")\r\n",
    "#components"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Each midi file contains notes and chords. These two properties will be the input and output of the LSTM network so \r\n",
    "# they need to be taken out from all midi files. \r\n",
    "\r\n",
    "def get_notes_per_song(path, filename, save_path, save_name):\r\n",
    "    \"\"\"\r\n",
    "    This function extracts all the notes, rests and chords from one midi file\r\n",
    "    and saves it in a list in the converted_data folder.\r\n",
    "\r\n",
    "    Param: Path of the midi file, filename (str)\r\n",
    "    \"\"\"\r\n",
    "    components = info_midi(path, filename)\r\n",
    "    note_list = []\r\n",
    "    \r\n",
    "    for element in components:\r\n",
    "        # note pitches are extracted\r\n",
    "        if isinstance(element, note.Note):\r\n",
    "            note_list.append(str(element.pitch))\r\n",
    "        # chords are extracted\r\n",
    "        elif isinstance(element, chord.Chord):\r\n",
    "            note_list.append(\".\".join(str(n) for n in element.normalOrder))    \r\n",
    "        # rests are extracted\r\n",
    "        elif isinstance(element, note.Rest):\r\n",
    "            note_list.append(\"NULL\")    #further transformation needs this value as str rather than np.nan\r\n",
    "\r\n",
    "    # save list with all componenets extracted\r\n",
    "    with open(save_path + save_name, \"wb\") as filepath:\r\n",
    "        pickle.dump(note_list, filepath)\r\n",
    "    \r\n",
    "    return note_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "one_song = get_notes_per_song(path_1, \"C_alb_esp1.mid\", path_2, \"one_notes\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def get_all_notes(path, save_name, save_path):\r\n",
    "    \"\"\"\r\n",
    "    This function extracts all the notes, rests and chords from all midi files \r\n",
    "    and saves it in a list in the converted_data folder.\r\n",
    "\r\n",
    "    Param: Path of the midi file     \r\n",
    "    \"\"\"\r\n",
    "    all_notes = []\r\n",
    "    list_path = os.listdir(path)\r\n",
    "    for filename in list_path:\r\n",
    "        output = get_notes_per_song(path, filename, save_path, save_name)\r\n",
    "        all_notes += output\r\n",
    "        \r\n",
    "    return all_notes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "all_notes = get_all_notes(path = path_1, save_path = path_2, save_name = \"all_notes\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Load notes and chords previously separated\r\n",
    "def load_notes (path, filename):\r\n",
    "    \"\"\"\r\n",
    "    Load the note list containing pitches, rests and chords.\r\n",
    "    \r\n",
    "    Param: Path of the saved note list, and its name as string\r\n",
    "    \"\"\"\r\n",
    "    with open(path + filename, \"rb\") as f:\r\n",
    "        loaded_notes = pickle.load(f)\r\n",
    "        return loaded_notes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "load_chopin = load_notes(path_2, \"notes_chopin\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def prepare_sequences(notes, min_note_occurence, sequence_length, step):\r\n",
    "    \"\"\" \r\n",
    "    This function creates the input and output sequences used by the neural network.\r\n",
    "    It returns the x and y of the model.\r\n",
    "\r\n",
    "    Param: \r\n",
    "        Note: List containing all notes, rests and chords\r\n",
    "        Sequence_length: Lenght of notes given to the model to help predict the next\r\n",
    "        Step: Step (int) between one input sequence and the next one\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # get all pitchnames\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    print('Total unique notes:', len(pitchnames))\r\n",
    "\r\n",
    "    # Calculate occurence\r\n",
    "    note_freq = {}\r\n",
    "    for elem in notes:\r\n",
    "        note_freq[elem] = note_freq.get(elem, 0) + 1\r\n",
    "\r\n",
    "    ignored_notes = set()\r\n",
    "    for k, v in note_freq.items():\r\n",
    "        if note_freq[k] < min_note_occurence:\r\n",
    "            ignored_notes.add(k)\r\n",
    "    \r\n",
    "    \r\n",
    "    print('Unique words before ignoring:', len(pitchnames))\r\n",
    "    print('Ignoring words with occurence <', min_note_occurence)\r\n",
    "    pitchnames = sorted(set(pitchnames) - ignored_notes)\r\n",
    "    print('Unique words after ignoring:', len(pitchnames))\r\n",
    "\r\n",
    "    # create a dictionary to convert pitches (strings) to integers\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))  # rests are included  \r\n",
    "\r\n",
    "    network_input = []\r\n",
    "    network_output = []\r\n",
    "\r\n",
    "    # create input sequences and the corresponding outputs\r\n",
    "    for i in range(0, len(notes) - sequence_length, step): \r\n",
    "        # remove ignored notes from the note list   \r\n",
    "        if len(set(notes[i: i+ sequence_length + 1]).intersection(ignored_notes)) == 0:\r\n",
    "            network_input.append(notes[i:i + sequence_length])\r\n",
    "            network_output.append(notes[i + sequence_length])\r\n",
    "    # array of zeros\r\n",
    "    x = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    y = np.zeros((len(network_input), len(pitchnames)))\r\n",
    "    # exchange note values for their integer-code\r\n",
    "    for i, sequence in enumerate(network_input):\r\n",
    "        for j, note in enumerate(sequence):\r\n",
    "            x[i, j, note_to_int[note]] = 1\r\n",
    "        y[i, note_to_int[network_output[i]]] = 1\r\n",
    "\r\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The length of each sequence will be 100 notes/chords for now. This means that to predict the next note in the sequence the network has the previous 100 notes to help make the prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "x, y = prepare_sequences(notes=load_chopin, min_note_occurence sequence_length=100, step=3)  # length y step pueden variar  \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total unique notes: 109\n",
      "Unique words before ignoring: 109\n",
      "Ignoring words with occurence < 1\n",
      "Unique words after ignoring: 109\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "print(x.shape)\r\n",
    "print(y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(474, 100, 109)\n",
      "(474, 109)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def generate_notes(notes, model, temperature=1.0):\r\n",
    "    \"\"\" \r\n",
    "    Generate notes from the neural network based on a sequence of notes \r\n",
    "    \"\"\"\r\n",
    "    # pick a random sequence from the input as a starting point for the prediction\r\n",
    "    start = np.random.randint(0, len(notes)-100-1)\r\n",
    "\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \r\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\r\n",
    "    \r\n",
    "    pattern = notes[start: (start+100)] \r\n",
    "    prediction_output = []\r\n",
    "    patterns = []\r\n",
    "\r\n",
    "    # generate 500 notes, roughly two minutes of music\r\n",
    "    for note_index in range(100):\r\n",
    "        prediction_input = np.zeros((1, 100, len(pitchnames)))\r\n",
    "        for j, note in enumerate(pattern):\r\n",
    "            prediction_input[0, j, note_to_int[note]] = 1.0\r\n",
    "        preds = model.predict(prediction_input, verbose=0)[0] \r\n",
    "        next_index = sample(preds, temperature=temperature)\r\n",
    "        next_note = int_to_note[next_index]\r\n",
    "\r\n",
    "        pattern = pattern[1:]\r\n",
    "        pattern.append(next_note)\r\n",
    "\r\n",
    "        prediction_output.append(next_note)\r\n",
    "\r\n",
    "        patterns.append(next_index)\r\n",
    "        #patterns = patterns[1:len(patterns)]\r\n",
    "\r\n",
    "    return prediction_output, patterns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "prediction_output, patterns = generate_notes(notes=load_chopin, model=g, temperature=1.0)\r\n",
    "print(prediction_output)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-fefddecd9488>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_notes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def sample(preds, temperature=1.0):\r\n",
    "    # helper function to sample an index from a probability array\r\n",
    "    preds = np.asarray(preds).astype(\"float64\")\r\n",
    "    preds = np.log(preds) / temperature\r\n",
    "    exp_preds = np.exp(preds)\r\n",
    "    preds = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, preds, 1)\r\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creation of the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are four different types of layers:\n",
    "\n",
    "LSTM layers is a Recurrent Neural Net layer that takes a sequence as an input and can return either sequences (return_sequences=True) or a matrix.\n",
    "\n",
    "Dropout layers are a regularisation technique that consists of setting a fraction of input units to 0 at each update during the training to prevent overfitting. The fraction is determined by the parameter used with the layer.\n",
    "\n",
    "Dense layers or fully connected layers is a fully connected neural network layer where each input node is connected to each output node.\n",
    "\n",
    "The Activation layer determines what activation function our neural network will use to calculate the output of a node."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def generator_model(num_units= x.shape[0], latent_dim=(x.shape[1], x.shape[2])):\r\n",
    "\r\n",
    "    model = Sequential()\r\n",
    "    model.add(LSTM(num_units, input_shape=latent_dim, return_sequences=True))\r\n",
    "    model.add(Dense(latent_dim[1]))\r\n",
    "    model.add(Activation(\"softmax\"))\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "g = generator_model()\r\n",
    "g.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 100, 474)          1107264   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100, 109)          51775     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 109)          0         \n",
      "=================================================================\n",
      "Total params: 1,159,039\n",
      "Trainable params: 1,159,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def prepare_sequences_gan(notes, sequence_length, step):\r\n",
    "    \"\"\" \r\n",
    "    Prepare the sequences used by the neural network \r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # get all pitchnames\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    print('Total unique notes:', len(pitchnames))\r\n",
    "\r\n",
    "    # create a dictionary to convert pitches (strings) to integers\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))  # rests are included  \r\n",
    "\r\n",
    "    network_input = []\r\n",
    "    network_output = []\r\n",
    "\r\n",
    "    #sequence_in = []\r\n",
    "    #sequence_out = []\r\n",
    "\r\n",
    "    # create input sequences and the corresponding outputs\r\n",
    "    for i in range(0, len(notes) - 2*sequence_length, step):    \r\n",
    "        network_input.append(notes[i:i + sequence_length])\r\n",
    "        network_output.append(notes[i + sequence_length : i + 2*sequence_length])\r\n",
    "        # exchange their values for their integer-code\r\n",
    "\r\n",
    "        # network_input.append([note_to_int[elem] for elem in sequence_in])\r\n",
    "        # network_output.append(note_to_int[sequence_out])\r\n",
    "\r\n",
    "    x = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    y = np.zeros((len(network_input), sequence_length, len(pitchnames)))\r\n",
    "    for i, sequence in enumerate(network_input):\r\n",
    "        for j, note in enumerate(sequence):\r\n",
    "            x[i, j, note_to_int[note]] = 1\r\n",
    "            y[i, j, note_to_int[network_output[i][j]]] = 1\r\n",
    "\r\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import gc\r\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4381"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "x_gan, y_gan = prepare_sequences_gan(notes=load_chopin, sequence_length=100, step=3)  # length y step pueden variar  \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total unique notes: 109\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "def generate_notes_gan(notes, model, temperature=1.0):\r\n",
    "    \"\"\" \r\n",
    "    Generate notes from the neural network based on a sequence of notes \r\n",
    "    \"\"\"\r\n",
    "    # pick a random sequence from the input as a starting point for the prediction\r\n",
    "    start = np.random.randint(0, len(notes)-100-1)\r\n",
    "\r\n",
    "    pitchnames = sorted(set(notes))\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \r\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\r\n",
    "    \r\n",
    "    pattern = notes[start: (start+100)] \r\n",
    "    prediction_output = []\r\n",
    "    patterns = []\r\n",
    "\r\n",
    "    # generate 500 notes, roughly two minutes of music\r\n",
    "\r\n",
    "    prediction_input = np.zeros((1, 100, len(pitchnames)))\r\n",
    "    for j, note in enumerate(pattern):\r\n",
    "        prediction_input[0, j, note_to_int[note]] = 1.0\r\n",
    "    preds = model.predict(prediction_input, verbose=0)[0]  \r\n",
    "\r\n",
    "    for elem in list(preds):\r\n",
    "        next_index = sample(elem, temperature=temperature)\r\n",
    "        next_note = int_to_note[next_index]\r\n",
    "        #pattern = pattern[1:]\r\n",
    "        #pattern.append(next_note)\r\n",
    "        prediction_output.append(next_note)\r\n",
    "\r\n",
    "        patterns.append(next_index)\r\n",
    "        #patterns = patterns[1:len(patterns)]\r\n",
    "\r\n",
    "    return prediction_output, patterns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "prediction_output, patterns = generate_notes_gan(load_chopin, g, temperature=1)\r\n",
    "print(prediction_output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['2.7', 'D3', 'C#5', 'A3', 'E2', '2.5', '0.5', 'E3', 'F4', '4.7.10.0', '7.0', '1.2', 'E6', '6.9.0', '8.11', 'NULL', '4.8.11', 'E-6', 'E3', 'C4', 'G#2', '4.7.10.0', 'C5', 'G#3', 'A5', '6.9.11', 'E-3', 'A5', '8.11', '4.8', '10.2', '6.9.11', 'A3', '9', '9.1', '0.5', 'A4', 'B2', '4.7.10.0', '7.0', 'B3', '2.7', '1.7', 'C3', '2.6', '5.7', 'C#6', '4.8', '9.0.4', 'D6', 'D6', 'E4', 'C#4', 'B2', '9.0.4', 'C#4', '9.0.4', '10.0', 'D2', 'A3', 'F4', '6.9', 'B-5', 'G#4', '8.11.2.4', 'G5', '2.7', '8.11', '10.2', 'B4', 'C#2', '4.8.11', '5.7', 'G2', 'B-2', 'A2', '9.0.4', '9.0.4', '2.5', '5.9.11', 'B4', '0.3.6', '9.2', '5.11', 'A4', '4.8.11', 'C#4', 'C4', 'G#5', 'F2', '4.7.9', 'G#3', 'C2', 'C#2', 'E2', '8.11.2', '4.7.10.0', '9.0.3', '1.7', 'B-5']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "len(prediction_output)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def create_midi(prediction_output, patterns, path):\r\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file from the notes\"\"\"\r\n",
    "    \r\n",
    "    offset = 0\r\n",
    "    output_notes = []\r\n",
    "\r\n",
    "    # create note and chord objects based on the values generated by the model\r\n",
    "    for pattern in prediction_output:\r\n",
    "        # pattern is a chord\r\n",
    "        if ('.' in pattern) or pattern.isdigit():\r\n",
    "            notes_in_chord = pattern.split('.')\r\n",
    "            notes = []\r\n",
    "            for current_note in notes_in_chord:\r\n",
    "                new_note = note.Note(int(current_note))\r\n",
    "                new_note.storedInstrument = instrument.Piano()\r\n",
    "                notes.append(new_note)\r\n",
    "            new_chord = chord.Chord(notes)\r\n",
    "            new_chord.offset = offset\r\n",
    "            output_notes.append(new_chord)\r\n",
    "        # pattern is a rest\r\n",
    "        elif (\"NULL\" in pattern):\r\n",
    "            new_rest = note.Rest(pattern)\r\n",
    "            output_notes.append(new_rest)\r\n",
    "        # pattern is a note\r\n",
    "        else:\r\n",
    "            new_note = note.Note(pattern)   \r\n",
    "            new_note.offset = offset\r\n",
    "            new_note.storedInstrument = instrument.Piano()\r\n",
    "            output_notes.append(new_note)\r\n",
    "\r\n",
    "        # increase offset each iteration so that notes do not stack\r\n",
    "        offset += 0.5\r\n",
    "\r\n",
    "    midi_stream = stream.Stream(output_notes)\r\n",
    "\r\n",
    "    midi_stream.write(\"midi\", fp= path + \"test_output_11_gmodel_chopin.mid\")   # first output 01/07/2021\r\n",
    "\r\n",
    "    return midi_stream"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "source": [
    "create_midi = create_midi(prediction_output, patterns, path_4)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def play_music(music_file):\r\n",
    "    \"\"\"\r\n",
    "    Play music given a midi file path\r\n",
    "    \"\"\"\r\n",
    "    import music21\r\n",
    "    try:\r\n",
    "        # allow to stop the piece \r\n",
    "        pygame.mixer.init()\r\n",
    "        clock = pygame.time.Clock() \r\n",
    "        pygame.mixer.music.load(music_file)\r\n",
    "        pygame.mixer.music.play()\r\n",
    "        while pygame.mixer.music.get_busy():\r\n",
    "            # check if playback has finished\r\n",
    "            clock.tick(10)\r\n",
    "\r\n",
    "        freq = 44100    # audio CD quality\r\n",
    "        bitsize = -16   # unsigned 16 bit\r\n",
    "        channels = 2    # 1 is mono, 2 is stereo\r\n",
    "        buffer = 1024    # number of samples\r\n",
    "        pygame.mixer.init(freq, bitsize, channels, buffer)\r\n",
    "\r\n",
    "    except KeyboardInterrupt:\r\n",
    "        while True:\r\n",
    "            action = input('Enter Q to Quit, Enter to Skip.').lower()\r\n",
    "            if action == 'q':\r\n",
    "                pygame.mixer.music.fadeout(1000)\r\n",
    "                pygame.mixer.music.stop()\r\n",
    "            else:\r\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "source": [
    "# Plays music when the cell is executed \r\n",
    "\r\n",
    "play_music(path_4 + \"test_output_11_gmodel_chopin.mid\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# hasta aquÃ­ va!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def generate_real_samples(x, n_samples):\r\n",
    "    \"\"\"\r\n",
    "    Load and prepare training notes\r\n",
    "    \"\"\"\r\n",
    "    # choose random instances\r\n",
    "    start = np.random.randint(0, len(x)-100-1)\r\n",
    "    # retrieve selected images\r\n",
    "    x_real = x[start: (start+n_samples)] \r\n",
    "    # generate 'real' class labels (1)\r\n",
    "    y_real = np.ones((n_samples, 1))\r\n",
    "\r\n",
    "    return x_real, y_real"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "x_real, y_real = generate_real_samples(x, n_samples=100)\r\n",
    "y_real.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def generate_latent_points(x, n_samples):\r\n",
    "    import random\r\n",
    "    # create random matrix of numbers \r\n",
    "    x_latent = np.zeros((n_samples, x.shape[1], x.shape[2]))\r\n",
    "  \r\n",
    "    for j, elem in enumerate(x_latent):\r\n",
    "        for k, row in enumerate(elem):\r\n",
    "            num = random.randint(0, x.shape[2])\r\n",
    "            for i in range(len(row)):\r\n",
    "                if i == num:\r\n",
    "                    x_latent[j][k][i] = 1\r\n",
    "\r\n",
    "    return x_latent\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "x_latent = generate_latent_points(x, n_samples=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def generate_fake_data(x, g_model, n_samples):\r\n",
    "\t# create 'fake' class labels (0)\r\n",
    "\ty_fake = np.zeros((n_samples, 1))\r\n",
    "\r\n",
    "\t# generate points in latent space\r\n",
    "\tx_latent = generate_latent_points(x, n_samples)\r\n",
    "\t# predict outputs\r\n",
    "\tx_fake = g_model.predict(x_latent, verbose=0)\r\n",
    "\r\n",
    "\treturn x_fake, y_fake\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "x_fake, y_fake = generate_fake_data(x, g, n_samples=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# define the standalone discriminator model\r\n",
    "def discriminator_model(n_inputs=(x.shape[1], x.shape[2])):\r\n",
    "\tmodel = Sequential()\r\n",
    "\tmodel.add(LSTM(512, input_shape=n_inputs))\r\n",
    "\tmodel.add(Dense(x.shape[2]))\r\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\r\n",
    "\t# compile model\r\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "\treturn model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "d = discriminator_model()\r\n",
    "d.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 512)               1273856   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 109)               55917     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 110       \n",
      "=================================================================\n",
      "Total params: 1,329,883\n",
      "Trainable params: 1,329,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def gan_model(g_model, d_model):\r\n",
    "\t# make weights in the discriminator not trainable\r\n",
    "\td_model.trainable = False\r\n",
    "\t# connect them\r\n",
    "\tmodel = Sequential()\r\n",
    "\t# add generator\r\n",
    "\tmodel.add(g_model)\r\n",
    "\tmodel.add(BatchNormalization())\r\n",
    "\t# add the discriminator\r\n",
    "\tmodel.add(d_model)\r\n",
    "\t# compile model\r\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=\"adam\")\r\n",
    "\treturn model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "gan_model = gan_model(g, d)\r\n",
    "gan_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 100, 109)          1329773   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 109)          436       \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 1329883   \n",
      "=================================================================\n",
      "Total params: 2,660,092\n",
      "Trainable params: 1,329,991\n",
      "Non-trainable params: 1,330,101\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# First, the discriminator model is updated for a half batch of real samples, then a half batch of fake samples, \r\n",
    "# together forming one batch of weight updates. The generator is then updated via the combined GAN model. \r\n",
    "# Importantly, the class label is set to 1 or real for the fake samples. This has the effect of updating the generator toward \r\n",
    "# getting better at generating real samples on the next batch."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# evaluate the discriminator, plot generated images, save generator model\r\n",
    "def check_performance(epoch, g_model, d_model, x, n_samples=100):\r\n",
    "\t# prepare real samples\r\n",
    "\tx_real, y_real = generate_real_samples(x, n_samples)\r\n",
    "\t# evaluate discriminator on real examples\r\n",
    "\t_, acc_real = d_model.evaluate(x_real, y_real, verbose=0)\r\n",
    "\t# prepare fake examples\r\n",
    "\tx_fake, y_fake = generate_fake_data(x, g, n_samples)\r\n",
    "\t# evaluate discriminator on fake examples\r\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\r\n",
    "\t# summarize discriminator performance\r\n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\r\n",
    "\t# save plot\r\n",
    "\t#save_plot(x_fake, epoch)\r\n",
    "\t# save the generator model tile file\r\n",
    "\t#filename = 'generator_model_%03d.h5' % (epoch+1)\r\n",
    "\t#g_model.save(filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "check_performance = check_performance(1, g, d, x, n_samples=100)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">Accuracy real: 41%, fake: 100%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# train the generator and discriminator\r\n",
    "def train(x, g_model, d_model, gan_model, y_gan, n_epochs, n_batch):\r\n",
    "\tbatch_per_epoch = int(x.shape[0]/n_batch)\r\n",
    "\thalf_batch = int(n_batch / 2)\r\n",
    "\t# manually enumerate epochs\r\n",
    "\tfor i in range(n_epochs):\r\n",
    "\t\t# enumerate batches over the training set\r\n",
    "\t\tfor j in range(batch_per_epoch):\r\n",
    "\t\t\t# get randomly selected 'real' samples\r\n",
    "\t\t\tx_real, y_real = generate_real_samples(x, n_samples=half_batch)\r\n",
    "\t\t\t# update discriminator model weights\r\n",
    "\t\t\td_loss1, _ = d_model.train_on_batch(x_real, y_real)\r\n",
    "\t\t\t# generate 'fake' examples \r\n",
    "\t\t\tx_fake, y_fake = generate_fake_data(x, g, n_samples=half_batch)\r\n",
    "\t\t\t# update discriminator model weights\r\n",
    "\t\t\td_loss2, _ = d_model.train_on_batch(x_fake, y_fake)\r\n",
    "\t\t\t# prepare points in latent space as input for the generator\r\n",
    "\t\t\tx_latent = generate_latent_points(x, n_samples=n_batch)\r\n",
    "\t\t\t# create inverted labels for the fake samples\r\n",
    "\t\t\ty_fake_1 = np.ones((n_batch, 1))\r\n",
    "\t\t\t# update the generator via the discriminator's error\r\n",
    "\t\t\tg_loss = gan_model.train_on_batch(x_latent, y_fake_1)\r\n",
    "\t\t\t# summarize loss on this batch\r\n",
    "\t\t\tprint('>%d, %d/%d, loss_real=%.3f, loss_fake=%.3f loss_latent=%.3f' %\r\n",
    "\t\t\t\t(i+1, j+1, batch_per_epoch, d_loss1, d_loss2, g_loss))\r\n",
    "\t\t# evaluate the model performance, sometimes\r\n",
    "\t\tif (n_epochs+1) % 10 == 0:\r\n",
    "\t\t\tcheck_performance(n_epochs, g_model, d_model, x, n_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "train(x=x, g_model=g, d_model=d, gan_model=gan_model, y_gan=y_gan, n_epochs=100, n_batch=129)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">1, 1/13, loss_real=0.693, loss_fake=0.729 loss_latent=0.682\n",
      ">1, 2/13, loss_real=0.567, loss_fake=0.684 loss_latent=0.680\n",
      ">1, 3/13, loss_real=0.580, loss_fake=0.489 loss_latent=0.040\n",
      ">1, 4/13, loss_real=4.156, loss_fake=0.316 loss_latent=0.535\n",
      ">1, 5/13, loss_real=0.160, loss_fake=0.599 loss_latent=0.550\n",
      ">1, 6/13, loss_real=0.524, loss_fake=0.634 loss_latent=0.538\n",
      ">1, 7/13, loss_real=0.541, loss_fake=0.638 loss_latent=0.504\n",
      ">1, 8/13, loss_real=0.578, loss_fake=0.626 loss_latent=0.430\n",
      ">1, 9/13, loss_real=0.416, loss_fake=0.584 loss_latent=0.103\n",
      ">1, 10/13, loss_real=0.005, loss_fake=0.427 loss_latent=0.000\n",
      ">1, 11/13, loss_real=0.000, loss_fake=0.088 loss_latent=0.000\n",
      ">1, 12/13, loss_real=0.000, loss_fake=0.007 loss_latent=0.000\n",
      ">1, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">2, 1/13, loss_real=1.108, loss_fake=0.001 loss_latent=0.000\n",
      ">2, 2/13, loss_real=0.100, loss_fake=0.002 loss_latent=0.000\n",
      ">2, 3/13, loss_real=0.000, loss_fake=0.005 loss_latent=0.000\n",
      ">2, 4/13, loss_real=0.000, loss_fake=0.009 loss_latent=0.000\n",
      ">2, 5/13, loss_real=0.000, loss_fake=0.013 loss_latent=0.000\n",
      ">2, 6/13, loss_real=0.515, loss_fake=0.023 loss_latent=0.000\n",
      ">2, 7/13, loss_real=1.099, loss_fake=0.075 loss_latent=0.799\n",
      ">2, 8/13, loss_real=0.438, loss_fake=0.290 loss_latent=0.000\n",
      ">2, 9/13, loss_real=0.200, loss_fake=0.480 loss_latent=0.000\n",
      ">2, 10/13, loss_real=0.000, loss_fake=0.359 loss_latent=0.000\n",
      ">2, 11/13, loss_real=0.113, loss_fake=0.201 loss_latent=0.000\n",
      ">2, 12/13, loss_real=0.000, loss_fake=0.109 loss_latent=0.000\n",
      ">2, 13/13, loss_real=0.041, loss_fake=0.060 loss_latent=0.000\n",
      ">3, 1/13, loss_real=0.859, loss_fake=0.044 loss_latent=0.000\n",
      ">3, 2/13, loss_real=1.657, loss_fake=0.058 loss_latent=0.000\n",
      ">3, 3/13, loss_real=1.245, loss_fake=0.113 loss_latent=0.000\n",
      ">3, 4/13, loss_real=0.147, loss_fake=0.196 loss_latent=0.000\n",
      ">3, 5/13, loss_real=0.301, loss_fake=0.274 loss_latent=0.000\n",
      ">3, 6/13, loss_real=0.235, loss_fake=0.331 loss_latent=0.000\n",
      ">3, 7/13, loss_real=0.000, loss_fake=0.349 loss_latent=0.000\n",
      ">3, 8/13, loss_real=0.000, loss_fake=0.325 loss_latent=0.000\n",
      ">3, 9/13, loss_real=0.000, loss_fake=0.274 loss_latent=0.000\n",
      ">3, 10/13, loss_real=0.000, loss_fake=0.212 loss_latent=0.000\n",
      ">3, 11/13, loss_real=0.576, loss_fake=0.166 loss_latent=0.000\n",
      ">3, 12/13, loss_real=0.315, loss_fake=0.139 loss_latent=0.000\n",
      ">3, 13/13, loss_real=0.000, loss_fake=0.115 loss_latent=0.000\n",
      ">4, 1/13, loss_real=0.032, loss_fake=0.090 loss_latent=0.000\n",
      ">4, 2/13, loss_real=0.040, loss_fake=0.069 loss_latent=0.000\n",
      ">4, 3/13, loss_real=0.214, loss_fake=0.054 loss_latent=0.000\n",
      ">4, 4/13, loss_real=0.457, loss_fake=0.046 loss_latent=0.000\n",
      ">4, 5/13, loss_real=0.291, loss_fake=0.044 loss_latent=0.000\n",
      ">4, 6/13, loss_real=0.000, loss_fake=0.043 loss_latent=0.000\n",
      ">4, 7/13, loss_real=0.911, loss_fake=0.047 loss_latent=0.000\n",
      ">4, 8/13, loss_real=0.041, loss_fake=0.056 loss_latent=0.000\n",
      ">4, 9/13, loss_real=0.936, loss_fake=0.072 loss_latent=0.000\n",
      ">4, 10/13, loss_real=0.000, loss_fake=0.094 loss_latent=0.000\n",
      ">4, 11/13, loss_real=0.447, loss_fake=0.116 loss_latent=0.000\n",
      ">4, 12/13, loss_real=0.000, loss_fake=0.135 loss_latent=0.000\n",
      ">4, 13/13, loss_real=0.000, loss_fake=0.139 loss_latent=0.000\n",
      ">5, 1/13, loss_real=0.000, loss_fake=0.132 loss_latent=0.000\n",
      ">5, 2/13, loss_real=0.617, loss_fake=0.127 loss_latent=0.000\n",
      ">5, 3/13, loss_real=0.129, loss_fake=0.126 loss_latent=0.000\n",
      ">5, 4/13, loss_real=0.421, loss_fake=0.127 loss_latent=0.000\n",
      ">5, 5/13, loss_real=0.148, loss_fake=0.127 loss_latent=0.000\n",
      ">5, 6/13, loss_real=0.000, loss_fake=0.120 loss_latent=0.000\n",
      ">5, 7/13, loss_real=0.123, loss_fake=0.107 loss_latent=0.000\n",
      ">5, 8/13, loss_real=0.694, loss_fake=0.103 loss_latent=0.000\n",
      ">5, 9/13, loss_real=0.393, loss_fake=0.111 loss_latent=0.000\n",
      ">5, 10/13, loss_real=0.211, loss_fake=0.120 loss_latent=0.000\n",
      ">5, 11/13, loss_real=0.531, loss_fake=0.134 loss_latent=0.000\n",
      ">5, 12/13, loss_real=0.384, loss_fake=0.160 loss_latent=0.000\n",
      ">5, 13/13, loss_real=0.586, loss_fake=0.206 loss_latent=0.000\n",
      ">6, 1/13, loss_real=0.000, loss_fake=0.244 loss_latent=0.000\n",
      ">6, 2/13, loss_real=0.052, loss_fake=0.245 loss_latent=0.000\n",
      ">6, 3/13, loss_real=0.051, loss_fake=0.228 loss_latent=0.000\n",
      ">6, 4/13, loss_real=0.224, loss_fake=0.209 loss_latent=0.000\n",
      ">6, 5/13, loss_real=0.000, loss_fake=0.167 loss_latent=0.000\n",
      ">6, 6/13, loss_real=0.000, loss_fake=0.108 loss_latent=0.000\n",
      ">6, 7/13, loss_real=0.000, loss_fake=0.063 loss_latent=0.000\n",
      ">6, 8/13, loss_real=0.000, loss_fake=0.035 loss_latent=0.000\n",
      ">6, 9/13, loss_real=0.000, loss_fake=0.020 loss_latent=0.000\n",
      ">6, 10/13, loss_real=0.223, loss_fake=0.012 loss_latent=0.000\n",
      ">6, 11/13, loss_real=0.000, loss_fake=0.009 loss_latent=0.000\n",
      ">6, 12/13, loss_real=0.677, loss_fake=0.009 loss_latent=0.000\n",
      ">6, 13/13, loss_real=1.349, loss_fake=0.018 loss_latent=0.000\n",
      ">7, 1/13, loss_real=0.769, loss_fake=0.044 loss_latent=0.000\n",
      ">7, 2/13, loss_real=0.049, loss_fake=0.089 loss_latent=0.000\n",
      ">7, 3/13, loss_real=0.119, loss_fake=0.136 loss_latent=0.000\n",
      ">7, 4/13, loss_real=0.000, loss_fake=0.170 loss_latent=0.000\n",
      ">7, 5/13, loss_real=0.000, loss_fake=0.177 loss_latent=0.000\n",
      ">7, 6/13, loss_real=0.000, loss_fake=0.160 loss_latent=0.000\n",
      ">7, 7/13, loss_real=0.000, loss_fake=0.132 loss_latent=0.000\n",
      ">7, 8/13, loss_real=0.018, loss_fake=0.103 loss_latent=0.000\n",
      ">7, 9/13, loss_real=0.000, loss_fake=0.077 loss_latent=0.000\n",
      ">7, 10/13, loss_real=0.209, loss_fake=0.059 loss_latent=0.000\n",
      ">7, 11/13, loss_real=0.040, loss_fake=0.046 loss_latent=0.000\n",
      ">7, 12/13, loss_real=0.340, loss_fake=0.039 loss_latent=0.000\n",
      ">7, 13/13, loss_real=0.488, loss_fake=0.039 loss_latent=0.000\n",
      ">8, 1/13, loss_real=0.385, loss_fake=0.050 loss_latent=0.000\n",
      ">8, 2/13, loss_real=0.156, loss_fake=0.069 loss_latent=0.000\n",
      ">8, 3/13, loss_real=0.000, loss_fake=0.087 loss_latent=0.000\n",
      ">8, 4/13, loss_real=0.000, loss_fake=0.097 loss_latent=0.000\n",
      ">8, 5/13, loss_real=0.034, loss_fake=0.095 loss_latent=0.000\n",
      ">8, 6/13, loss_real=0.000, loss_fake=0.084 loss_latent=0.000\n",
      ">8, 7/13, loss_real=0.000, loss_fake=0.068 loss_latent=0.000\n",
      ">8, 8/13, loss_real=0.498, loss_fake=0.146 loss_latent=0.000\n",
      ">8, 9/13, loss_real=0.011, loss_fake=23.064 loss_latent=0.000\n",
      ">8, 10/13, loss_real=0.000, loss_fake=5.372 loss_latent=4.856\n",
      ">8, 11/13, loss_real=4.744, loss_fake=0.148 loss_latent=3.178\n",
      ">8, 12/13, loss_real=2.247, loss_fake=0.535 loss_latent=0.905\n",
      ">8, 13/13, loss_real=0.849, loss_fake=0.761 loss_latent=1.241\n",
      ">9, 1/13, loss_real=0.689, loss_fake=0.587 loss_latent=1.174\n",
      ">9, 2/13, loss_real=0.426, loss_fake=2.322 loss_latent=0.137\n",
      ">9, 3/13, loss_real=1.514, loss_fake=0.437 loss_latent=0.295\n",
      ">9, 4/13, loss_real=0.842, loss_fake=0.964 loss_latent=0.623\n",
      ">9, 5/13, loss_real=0.562, loss_fake=0.683 loss_latent=0.830\n",
      ">9, 6/13, loss_real=0.594, loss_fake=0.596 loss_latent=0.873\n",
      ">9, 7/13, loss_real=0.621, loss_fake=0.513 loss_latent=0.965\n",
      ">9, 8/13, loss_real=0.641, loss_fake=0.495 loss_latent=1.028\n",
      ">9, 9/13, loss_real=0.358, loss_fake=0.455 loss_latent=1.107\n",
      ">9, 10/13, loss_real=1.316, loss_fake=0.422 loss_latent=1.572\n",
      ">9, 11/13, loss_real=0.873, loss_fake=0.147 loss_latent=2.324\n",
      ">9, 12/13, loss_real=0.202, loss_fake=0.020 loss_latent=0.090\n",
      ">9, 13/13, loss_real=0.065, loss_fake=0.181 loss_latent=5.650\n",
      ">10, 1/13, loss_real=5.748, loss_fake=0.045 loss_latent=0.960\n",
      ">10, 2/13, loss_real=1.432, loss_fake=0.273 loss_latent=0.565\n",
      ">10, 3/13, loss_real=0.951, loss_fake=0.392 loss_latent=0.662\n",
      ">10, 4/13, loss_real=0.828, loss_fake=0.455 loss_latent=0.411\n",
      ">10, 5/13, loss_real=0.755, loss_fake=0.461 loss_latent=0.636\n",
      ">10, 6/13, loss_real=0.844, loss_fake=0.467 loss_latent=0.709\n",
      ">10, 7/13, loss_real=0.735, loss_fake=0.464 loss_latent=0.621\n",
      ">10, 8/13, loss_real=0.638, loss_fake=0.458 loss_latent=0.220\n",
      ">10, 9/13, loss_real=0.607, loss_fake=0.416 loss_latent=0.241\n",
      ">10, 10/13, loss_real=0.676, loss_fake=0.384 loss_latent=0.153\n",
      ">10, 11/13, loss_real=0.226, loss_fake=0.400 loss_latent=0.223\n",
      ">10, 12/13, loss_real=0.174, loss_fake=0.333 loss_latent=0.220\n",
      ">10, 13/13, loss_real=0.190, loss_fake=0.213 loss_latent=0.069\n",
      ">11, 1/13, loss_real=0.165, loss_fake=0.139 loss_latent=0.014\n",
      ">11, 2/13, loss_real=0.058, loss_fake=0.087 loss_latent=0.004\n",
      ">11, 3/13, loss_real=0.013, loss_fake=0.051 loss_latent=0.001\n",
      ">11, 4/13, loss_real=0.013, loss_fake=0.029 loss_latent=0.000\n",
      ">11, 5/13, loss_real=0.006, loss_fake=0.017 loss_latent=0.000\n",
      ">11, 6/13, loss_real=0.001, loss_fake=0.010 loss_latent=0.000\n",
      ">11, 7/13, loss_real=0.001, loss_fake=0.007 loss_latent=0.000\n",
      ">11, 8/13, loss_real=0.001, loss_fake=0.004 loss_latent=0.000\n",
      ">11, 9/13, loss_real=0.000, loss_fake=0.003 loss_latent=0.000\n",
      ">11, 10/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">11, 11/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">11, 12/13, loss_real=0.323, loss_fake=0.004 loss_latent=0.000\n",
      ">11, 13/13, loss_real=0.000, loss_fake=0.016 loss_latent=0.000\n",
      ">12, 1/13, loss_real=0.000, loss_fake=7.112 loss_latent=0.002\n",
      ">12, 2/13, loss_real=0.011, loss_fake=0.032 loss_latent=0.420\n",
      ">12, 3/13, loss_real=0.318, loss_fake=0.026 loss_latent=0.796\n",
      ">12, 4/13, loss_real=0.408, loss_fake=0.020 loss_latent=0.840\n",
      ">12, 5/13, loss_real=0.264, loss_fake=0.015 loss_latent=0.848\n",
      ">12, 6/13, loss_real=0.189, loss_fake=0.010 loss_latent=0.335\n",
      ">12, 7/13, loss_real=0.083, loss_fake=0.007 loss_latent=0.058\n",
      ">12, 8/13, loss_real=0.031, loss_fake=0.006 loss_latent=0.013\n",
      ">12, 9/13, loss_real=0.010, loss_fake=0.005 loss_latent=0.005\n",
      ">12, 10/13, loss_real=0.004, loss_fake=0.004 loss_latent=0.002\n",
      ">12, 11/13, loss_real=0.002, loss_fake=0.004 loss_latent=0.001\n",
      ">12, 12/13, loss_real=0.001, loss_fake=0.003 loss_latent=0.001\n",
      ">12, 13/13, loss_real=0.001, loss_fake=0.003 loss_latent=0.001\n",
      ">13, 1/13, loss_real=0.001, loss_fake=0.003 loss_latent=0.001\n",
      ">13, 2/13, loss_real=0.001, loss_fake=0.003 loss_latent=0.000\n",
      ">13, 3/13, loss_real=0.001, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 4/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 5/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 6/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 7/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 8/13, loss_real=0.079, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 9/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 10/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 11/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 12/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">13, 13/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 1/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 2/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 3/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 4/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 5/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 6/13, loss_real=0.081, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 7/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 8/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 9/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 10/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 11/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 12/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">14, 13/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 1/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 2/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 3/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 4/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 5/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 6/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">15, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">15, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">15, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">15, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">15, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">15, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">15, 13/13, loss_real=0.087, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 5/13, loss_real=0.083, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">16, 7/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">16, 8/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">16, 9/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">16, 10/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">16, 11/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">16, 12/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">16, 13/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">17, 1/13, loss_real=0.000, loss_fake=0.002 loss_latent=0.000\n",
      ">17, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">17, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">18, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">19, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">20, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">21, 9/13, loss_real=0.101, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">21, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">22, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">23, 12/13, loss_real=0.099, loss_fake=0.001 loss_latent=0.000\n",
      ">23, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">24, 13/13, loss_real=0.093, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">25, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 2/13, loss_real=0.088, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">26, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">27, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 4/13, loss_real=0.088, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">28, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 7/13, loss_real=0.083, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">29, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">30, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 7/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 8/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 9/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 10/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 11/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 12/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">31, 13/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 1/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 2/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 3/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 4/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 5/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 6/13, loss_real=0.000, loss_fake=0.001 loss_latent=0.000\n",
      ">32, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">32, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">32, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">32, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">32, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">32, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">32, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">33, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">34, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">35, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">36, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">37, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">38, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">39, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">40, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">41, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">42, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">43, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">44, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">45, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">46, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">47, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">48, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">49, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">50, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">51, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">52, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">53, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">54, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">55, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">56, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">57, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">58, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">59, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">60, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">61, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">62, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">63, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">64, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">65, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">66, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">67, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">68, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">69, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">70, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">71, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">72, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">73, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">74, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">75, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">76, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">77, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">78, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">79, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">80, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">81, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">82, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">83, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">84, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">85, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">86, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">87, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">88, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">89, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">90, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">91, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">92, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">93, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">94, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">95, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">96, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">97, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">98, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">99, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 1/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 2/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 3/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 4/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 5/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 6/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 7/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 8/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 9/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 10/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 11/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 12/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n",
      ">100, 13/13, loss_real=0.000, loss_fake=0.000 loss_latent=0.000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# aun no arreglado\r\n",
    "\r\n",
    "# evaluate the discriminator, plot generated images, save generator model\r\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\r\n",
    "\t# prepare real samples\r\n",
    "\tX_real, y_real = generate_real_samples(dataset, n_samples)\r\n",
    "\t# evaluate discriminator on real examples\r\n",
    "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\r\n",
    "\t# prepare fake examples\r\n",
    "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\r\n",
    "\t# evaluate discriminator on fake examples\r\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\r\n",
    "\t# summarize discriminator performance\r\n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\r\n",
    "\t# save plot\r\n",
    "\tsave_plot(x_fake, epoch)\r\n",
    "\t# save the generator model tile file\r\n",
    "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\r\n",
    "\tg_model.save(filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# save the model\r\n",
    "g.save(path_3 + \"model_g_4.h5\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "# load the model \r\n",
    "model_gan_3 = tf.keras.models.load_model(path_3 + \"model_g_4.h5\") # 3 - 100 epochs, 139 batches\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "#gan_model_1 = tf.keras.models.load_model(path_3 + \"model_g_3.h5\") \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "source": [
    "def sample(preds, temperature=1.0):\r\n",
    "    # helper function to sample an index from a probability array\r\n",
    "    preds = np.asarray(preds).astype(\"float64\")\r\n",
    "    preds = np.log(preds) / temperature\r\n",
    "    exp_preds = np.exp(preds)\r\n",
    "    preds = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, preds, 1)\r\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def generate_notes(model, note_list, temperature=1.0):\r\n",
    "    \"\"\" \r\n",
    "    Generate notes from the neural network based on a sequence of notes \r\n",
    "    \"\"\"\r\n",
    "    # pick a random sequence from the input as a starting point for the prediction\r\n",
    "    start = np.random.randint(0, len(note_list)-100-1)\r\n",
    "\r\n",
    "    pitchnames = sorted(set(note_list))\r\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \r\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\r\n",
    "    \r\n",
    "    pattern = note_list[start: (start+100)] \r\n",
    "    prediction_output = []\r\n",
    "    patterns = []\r\n",
    "\r\n",
    "    # generate 500 notes, roughly two minutes of music\r\n",
    "    for note_index in range(10):\r\n",
    "        prediction_input = np.zeros((1, 100, len(pitchnames)))\r\n",
    "        for j, note in enumerate(pattern):\r\n",
    "            prediction_input[0, j, note_to_int[note]] = 1.0 \r\n",
    "        preds = model.predict(prediction_input, verbose=0)[0]\r\n",
    "        for elem in list(preds):\r\n",
    "            next_index = sample(elem, temperature=temperature)\r\n",
    "            next_note = int_to_note[next_index]\r\n",
    "\r\n",
    "            pattern = pattern[1:]\r\n",
    "            pattern.append(next_note)\r\n",
    "\r\n",
    "            prediction_output.append(next_note)\r\n",
    "\r\n",
    "            patterns.append(next_index)\r\n",
    "            #patterns = patterns[1:len(patterns)]\r\n",
    "\r\n",
    "    return prediction_output, patterns, next_index, pattern"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "prediction_output, patterns, next_index, pattern = generate_notes(model_gan_3, note_list, temperature=1)\r\n",
    "print(prediction_output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['11.2.5.7', 'E4', '3.6', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '1', 'G4', '0', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '1.3', '10.1.4', '3.6', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '7.10.2', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '5.8.10', '7.11.2', 'G#6', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '7.10.2', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-4', 'G6', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '7.0', '3.6.9', 'F#4', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'G6', 'G3', 'F#3', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '7.10.2', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'E4', '1.6', 'B6', '7.10.2', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '7.10.2', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', '7.0', '4.7.10', '1.5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'E-7', '9.1', '3.6', '7.10.2', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5', 'B-5']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "def create_midi(prediction_output, patterns, path):\r\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file from the notes\"\"\"\r\n",
    "    \r\n",
    "    offset = 0\r\n",
    "    output_notes = []\r\n",
    "\r\n",
    "    # create note and chord objects based on the values generated by the model\r\n",
    "    for pattern in prediction_output:\r\n",
    "        # pattern is a chord\r\n",
    "        if ('.' in pattern) or pattern.isdigit():\r\n",
    "            notes_in_chord = pattern.split('.')\r\n",
    "            notes = []\r\n",
    "            for current_note in notes_in_chord:\r\n",
    "                new_note = note.Note(int(current_note))\r\n",
    "                new_note.storedInstrument = instrument.Piano()\r\n",
    "                notes.append(new_note)\r\n",
    "            new_chord = chord.Chord(notes)\r\n",
    "            new_chord.offset = offset\r\n",
    "            output_notes.append(new_chord)\r\n",
    "        # pattern is a rest\r\n",
    "        elif (\"NULL\" in pattern):\r\n",
    "            new_rest = note.Rest(pattern)\r\n",
    "            output_notes.append(new_rest)\r\n",
    "        # pattern is a note\r\n",
    "        else:\r\n",
    "            new_note = note.Note(pattern)   \r\n",
    "            new_note.offset = offset\r\n",
    "            new_note.storedInstrument = instrument.Piano()\r\n",
    "            output_notes.append(new_note)\r\n",
    "\r\n",
    "        # increase offset each iteration so that notes do not stack\r\n",
    "        offset += 0.5\r\n",
    "\r\n",
    "    midi_stream = stream.Stream(output_notes)\r\n",
    "\r\n",
    "    midi_stream.write(\"midi\", fp= path + \"test_output_13gen_model.mid\")   # first output 01/07/2021\r\n",
    "\r\n",
    "    return midi_stream"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "create_midi = create_midi(prediction_output, patterns, path_4)\r\n",
    "# 13gen - 100 epoch, 129 batches"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "def play_music(music_file):\r\n",
    "    \"\"\"\r\n",
    "    Play music given a midi file path\r\n",
    "    \"\"\"\r\n",
    "    import music21\r\n",
    "    try:\r\n",
    "        # allow to stop the piece \r\n",
    "        pygame.mixer.init()\r\n",
    "        clock = pygame.time.Clock() \r\n",
    "        pygame.mixer.music.load(music_file)\r\n",
    "        pygame.mixer.music.play()\r\n",
    "        while pygame.mixer.music.get_busy():\r\n",
    "            # check if playback has finished\r\n",
    "            clock.tick(10)\r\n",
    "\r\n",
    "        freq = 44100    # audio CD quality\r\n",
    "        bitsize = -16   # unsigned 16 bit\r\n",
    "        channels = 2    # 1 is mono, 2 is stereo\r\n",
    "        buffer = 1024    # number of samples\r\n",
    "        pygame.mixer.init(freq, bitsize, channels, buffer)\r\n",
    "\r\n",
    "    except KeyboardInterrupt:\r\n",
    "        while True:\r\n",
    "            action = input('Enter Q to Quit, Enter to Skip.').lower()\r\n",
    "            if action == 'q':\r\n",
    "                pygame.mixer.music.fadeout(1000)\r\n",
    "                pygame.mixer.music.stop()\r\n",
    "            else:\r\n",
    "                break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# Plays music when the cell is executed \r\n",
    "\r\n",
    "play_music(path_4 + \"test_output_12gen_model.mid\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8a740277f67c33143a8e5c8e55f738530a350d8def4a85d8635b690074994c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}